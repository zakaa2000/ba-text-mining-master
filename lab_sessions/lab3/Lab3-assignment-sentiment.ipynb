{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab3 - Assignment Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook describes the LAB-2 assignment of the Text Mining course. It is about sentiment analysis.\n",
    "\n",
    "The aims of the assignment are:\n",
    "* Learn how to run a rule-based sentiment analysis module (VADER)\n",
    "* Learn how to run a machine learning sentiment analysis module (Scikit-Learn/ Naive Bayes)\n",
    "* Learn how to run scikit-learn metrics for the quantitative evaluation\n",
    "* Learn how to perform and interpret a quantitative evaluation of the outcomes of the tools (in terms of Precision, Recall, and F<sub>1</sub>)\n",
    "* Learn how to evaluate the results qualitatively (by examining the data) \n",
    "* Get insight into differences between the two applied methods\n",
    "* Get insight into the effects of using linguistic preprocessing\n",
    "* Be able to describe differences between the two methods in terms of their results\n",
    "* Get insight into issues when applying these methods across different  domains\n",
    "\n",
    "In this assignment, you are going to create your own gold standard set from 50 tweets. You will the VADER and scikit-learn classifiers to these tweets and evaluate the results by using evaluation metrics and inspecting the data.\n",
    "\n",
    "We recommend you go through the notebooks in the following order:\n",
    "* **Read the assignment (see below)**\n",
    "* **Lab3.2-Sentiment-analysis-with-VADER.ipynb**\n",
    "* **Lab3.3-Sentiment-analysis.with-scikit-learn.ipynb**\n",
    "* **Answer the questions of the assignment (see below) using the provided notebooks and submit**\n",
    "\n",
    "In this assignment you are asked to perform both quantitative evaluations and error analyses:\n",
    "* a quantitative evaluation concerns the scores (Precision, Recall, and F<sub>1</sub>) provided by scikit's classification_report. It includes the scores per category, as well as micro and macro averages. Discuss whether the scores are balanced or not between the different categories (positive, negative, neutral) and between precision and recall. Discuss the shortcomings (if any) of the classifier based on these scores\n",
    "* an error analysis regarding the misclassifications of the classifier. It involves going through the texts and trying to understand what has gone wrong. It servers to get insight in what could be done to improve the performance of the classifier. Do you observe patterns in misclassifications?  Discuss why these errors are made and propose ways to solve them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits\n",
    "The notebooks in this block have been originally created by [Marten Postma](https://martenpostma.github.io) and [Isa Maks](https://research.vu.nl/en/persons/e-maks). Adaptations were made by [Filip Ilievski](http://ilievski.nl)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part I: VADER assignments\n",
    "\n",
    "\n",
    "### Preparation (nothing to submit):\n",
    "To be able to answer the VADER questions you need to know how the tool works. \n",
    "* Read more about the VADER tool in [this blog](http://t-redactyl.io/blog/2017/04/using-vader-to-handle-sentiment-analysis-with-social-media-text.html).  \n",
    "* VADER provides 4 scores (positive, negative, neutral, compound). Be sure to understand what they mean and how they are calculated.\n",
    "* VADER uses rules to handle linguistic phenomena such as negation and intensification. Be sure to understand which rules are used, how they work, and why they are important.\n",
    "* VADER makes use of a sentiment lexicon. Have a look at the lexicon. Be sure to understand which information can be found there (lemma?, wordform?, part-of-speech?, polarity value?, word meaning?) What do all scores mean? https://github.com/cjhutto/vaderSentiment/blob/master/vaderSentiment/vader_lexicon.txt) \n",
    "\n",
    "\n",
    "### [3.5 points] Question1:\n",
    "\n",
    "Regard the following sentences and their output as given by VADER. Regard sentences 1 to 7, and explain the outcome **for each sentence**. Take into account both the rules applied by VADER and the lexicon that is used. You will find that some of the results are reasonable, but others are not. Explain what is going wrong or not when correct and incorrect results are produced. \n",
    "\n",
    "```\n",
    "INPUT SENTENCE 1 I love apples\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.192, 'pos': 0.808, 'compound': 0.6369}\n",
    "\n",
    "INPUT SENTENCE 2 I don't love apples\n",
    "VADER OUTPUT {'neg': 0.627, 'neu': 0.373, 'pos': 0.0, 'compound': -0.5216}\n",
    "\n",
    "INPUT SENTENCE 3 I love apples :-)\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.133, 'pos': 0.867, 'compound': 0.7579}\n",
    "\n",
    "INPUT SENTENCE 4 These houses are ruins\n",
    "VADER OUTPUT {'neg': 0.492, 'neu': 0.508, 'pos': 0.0, 'compound': -0.4404}\n",
    "\n",
    "INPUT SENTENCE 5 These houses are certainly not considered ruins\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.51, 'pos': 0.49, 'compound': 0.5867}\n",
    "\n",
    "INPUT SENTENCE 6 He lies in the chair in the garden\n",
    "VADER OUTPUT {'neg': 0.286, 'neu': 0.714, 'pos': 0.0, 'compound': -0.4215}\n",
    "\n",
    "INPUT SENTENCE 7 This house is like any house\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.667, 'pos': 0.333, 'compound': 0.3612}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"The sentences with less reasonable scores are the second, third, fourth,fith and sixth. This is because:\n",
    "\n",
    "- In the second sentence the neutral feeling should be higher than the negative one, since the person don't hate apples but nor love them. \n",
    "- The third sentence should have the neutral feeling in 0.0, since it's explicit is a positive comment.\n",
    "- The fourth sentence can be interpreted in two ways, namely: the houses are old and dirty which indeed has a negative tone/score. However, it could also be interpreted as literal ruins. Then the score should be more neutral.\n",
    "- The fifth sentence should be neutral mostly, since it's not considered nor positive or negative comment.\n",
    "- The sixth comment should be neutral, and have a negative feeling of 0.0\n",
    "- The seventh sentence should have a lower positive score and rather little more negative. The tone of this sentence implies that the house that is being observed is not something special.\n",
    "\n",
    "Last sentences have these errors because not all parts of speech are provided. It's also important to put negations as the following \"These houses are certainly {not} considered ruins \" to the list of negation tokens, so it should appropriately flip the sentiment polarity \n",
    "while maintaining the intensity calculations. Most sentences lack of them.\n",
    "\n",
    "Additionally, it is needed to convert titles (headlines) into strings before applying polarity_scores function.\"\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Points: 2.5] Exercise 2: Collecting 50 tweets for evaluation\n",
    "Collect 50 tweets. Try to find tweets that are interesting for sentiment analysis, e.g., very positive, neutral, and negative tweets. These could be your own tweets (typed in) or collected from the Twitter stream.\n",
    "\n",
    "We will store the tweets in the file **my_tweets.json** (use a text editor to edit).\n",
    "For each tweet, you should insert:\n",
    "* sentiment analysis label: negative | neutral | positive (this you determine yourself, this is not done by a computer)\n",
    "* the text of the tweet\n",
    "* the Tweet-URL\n",
    "\n",
    "from:\n",
    "```\n",
    "    \"1\": {\n",
    "        \"sentiment_label\": \"\",\n",
    "        \"text_of_tweet\": \"\",\n",
    "        \"tweet_url\": \"\",\n",
    "```\n",
    "to:\n",
    "```\n",
    "\"1\": {\n",
    "        \"sentiment_label\": \"positive\",\n",
    "        \"text_of_tweet\": \"All across America people chose to get involved, get engaged and stand up. Each of us can make a difference, and all of us ought to try. So go keep changing the world in 2018.\",\n",
    "        \"tweet_url\" : \"https://twitter.com/BarackObama/status/946775615893655552\",\n",
    "    },\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can load your tweets with human annotation in the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tweets = json.load(open('my_tweets.json', encoding='utf8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 {'sentiment_label': 'negative', 'text_of_tweet': 'The cybersecurity industry is so very embarrassing.', 'tweet_url': 'https://mobile.twitter.com/GossiTheDog/status/1497899214637912066?cxt=HHwWhMCi3cjZzckpAAAA '}\n",
      "2 {'sentiment_label': 'positive', 'text_of_tweet': \"This is Donut. He wanted to pop out and say hi. Wondering where your personal chauffeur is. Said you can borrow his, if you'd like. 13/10\", 'tweet_url': 'https://mobile.twitter.com/dog_rates/status/1496265582550863879?cxt=HHwWjsCymdXn5sMpAAAA '}\n",
      "3 {'sentiment_label': 'negative', 'text_of_tweet': 'Ukraine’s ambassador to the US just told us that a Russian platoon from the 74th Motorized Brigade has surrendered to Ukraine’s forces. She says that the Russian troops apparently had been unaware they were being sent to kill Ukrainians. No confirmation yet from Russia’s military', 'tweet_url': 'https://mobile.twitter.com/JoshNBCNews/status/1496884570716835840?cxt=HHwWgMC4tcKlgMYpAAAA '}\n",
      "4 {'sentiment_label': 'negative', 'text_of_tweet': 'Long lines formed at a bus station in Kyiv after Russian attacks were launched across Ukraine. Residents waited in line for up to an hour to buy fuel. The city’s main airport was bombed and its major roads were jammed with traffic as people fled.', 'tweet_url': 'https://mobile.twitter.com/nytimes/status/1496814689325654016?cxt=HHwWgIC9vfHB4MUpAAAA'}\n",
      "5 {'sentiment_label': 'negative', 'text_of_tweet': 'Media Spreads Misinformation On Joe Rogan To Prevent Him From Spreading Misinformation', 'tweet_url': 'https://mobile.twitter.com/TheBabylonBee/status/1490803206523338755?cxt=HHwWhoCyrcTnsrApAAAA'}\n",
      "6 {'sentiment_label': 'negative', 'text_of_tweet': 'As you noticed no stream today. On my way to Poland to get to the Ukrainian border and get some refugees to Sweden.', 'tweet_url': 'https://mobile.twitter.com/Qullamaggie/status/1499814429038424067'}\n",
      "7 {'sentiment_label': 'negative', 'text_of_tweet': 'This man drove to Poland from DENMARK to help Ukrainian women and children. He said he is going back with six people, whom he will help settle in a new country, over 2000 km. away from home. “Putin is a dictator, and we don’t like that in Denmark,” he said.', 'tweet_url': 'https://mobile.twitter.com/lapatina_/status/1499411581813374982'}\n",
      "8 {'sentiment_label': 'neutral', 'text_of_tweet': 'sometimes i remember all those private qrts i’ve gotten on my popular tweets and a lot of them are most likely talking shit about me. that’s wild to think about', 'tweet_url': 'https://mobile.twitter.com/EASEFORALL/status/1500344726511243267'}\n",
      "9 {'sentiment_label': 'positive', 'text_of_tweet': 'I’m slowly doing a BIG archival of posts from over 13 years ago. The most memorable / popular posts. It’s a process. Thank you for adding me, remember that I am very much actively posting CURRENT stuff +  I put JDM under umbrella “MIVIDA JDM”', 'tweet_url': ' https://mobile.twitter.com/mividajdmtweets/status/1500475190215688199'}\n",
      "10 {'sentiment_label': 'positive', 'text_of_tweet': 'ata from the #Sentinel5P satellite are being used to detect emission hotspots in various regions, identifying #methane concentrations and locating exact facilities leaking methane', 'tweet_url': 'https://mobile.twitter.com/esa/status/1267724418433851392'}\n",
      "11 {'sentiment_label': 'positive', 'text_of_tweet': 'in 2017, Dorothy Vaughan, Katherine Johnson, and Mary Jackson, African-American women whose mathematical contributions made landing on the Moon possible, were inducted into the Langley Hall of Honor. Read more about their legacy below:', 'tweet_url': 'https://mobile.twitter.com/NASAhistory/status/1267531363017723904'}\n",
      "12 {'sentiment_label': 'negative', 'text_of_tweet': 'Study shows seven of the regions that dominate global ice mass losses are melting at an accelerated rate and are depleting freshwater resources that could potentially affect coastlines, agriculture & drinking water supplies in communities around the world. ', 'tweet_url': 'https://mobile.twitter.com/NASAJPL/status/1267495668861399040'}\n",
      "13 {'sentiment_label': 'negative', 'text_of_tweet': 'One of the dangers humans face when exploring #Mars is the toxic chemicals in the soil. However, these chemicals could also be key to producing usable oxygen.', 'tweet_url': 'https://mobile.twitter.com/DigitalTrends/status/1500478443183505409'}\n",
      "14 {'sentiment_label': 'neutral', 'text_of_tweet': 'Deciding between security cameras or video doorbells can be tough. We cover the pros and cons of installation, video coverage, and price.', 'tweet_url': 'https://mobile.twitter.com/DigitalTrends/status/1500119327856496640'}\n",
      "15 {'sentiment_label': 'positive', 'text_of_tweet': 'For years you’ve heard and read the many rumors suggesting a potential return for the iconic 1980s DeLorean gullwing, stainless-steel-bodied sports-car, and 2022 looks like the year it may finally show face.', 'tweet_url': 'https://mobile.twitter.com/MotorTrend/status/1498455494834409472'}\n",
      "16 {'sentiment_label': 'neutral', 'text_of_tweet': 'The James Carey Urban Communication Grant supports research that advances knowledge on the consequences of urban communication in and across urban societies. The grant is awarded by the Urban Communication Foundation (UCF) and co-sponsored', 'tweet_url': 'https://mobile.twitter.com/ICAPopular'}\n",
      "17 {'sentiment_label': 'negative', 'text_of_tweet': 'Twitter aggressively pushing ads and random popular tweets on my YL instead of letting me see the tweets of people I actually follow and it’s fucking annoying.', 'tweet_url': 'https://mobile.twitter.com/ScaryCleve/status/1500448704129548288'}\n",
      "18 {'sentiment_label': 'neutral', 'text_of_tweet': '“What’s the trend of the market? If it’s negative, you’ll want to do very little, if any buying, even if you see some stocks breaking out. Your probabilities of success are quite low when the market trend is going against you.”Stan Weinstein', 'tweet_url': 'https://mobile.twitter.com/rahuldmehta7/status/1499074907656888325'}\n",
      "19 {'sentiment_label': 'neutral/negative', 'text_of_tweet': 'Political counterparty risk has now been attached to every Dollar and every Euro on Earth. Buy Gold while there is still physical available.', 'tweet_url': 'https://mobile.twitter.com/TheLastDegree/status/1500450267493507075'}\n",
      "20 {'sentiment_label': 'negative/neutral', 'text_of_tweet': 'Ran for a train today for the first time ever. My mcm better tweet “if they wanted, to they would”', 'tweet_url': 'https://mobile.twitter.com/N3XNA/status/1500486348960063489'}\n",
      "21 {'sentiment_label': 'negative', 'text_of_tweet': 'My mistery: hyperglycemia, not yet diabete, low insulin level, low pep-c level, no D1 antibody, no mody, pancreas ok…no doctor has an explanation…any idea? Thanks', 'tweet_url': 'https://twitter.com/TMLitalia/status/1499408379021770752'}\n",
      "22 {'sentiment_label': 'neutral/negative', 'text_of_tweet': 'Events of the last two years highlight the extreme danger presented by centralization. The intellectual backing for the idea that we should have more of it rests on the notion that having more data enables better centralized decision-making. /1', 'tweet_url': 'https://twitter.com/NickHudsonCT/status/1500368650536169475'}\n",
      "23 {'sentiment_label': 'positive', 'text_of_tweet': \"Excellent discussion covering the intersections between COVID-19 and geopolitical crisis with Professor Isa Blumi and Jesse Zurawell. Essential listening. NOTE: It's the 4th interview down on the list, dated 1 March 2022\", 'tweet_url': 'https://twitter.com/PanData19/status/1500087790532542464'}\n",
      "24 {'sentiment_label': 'negative', 'text_of_tweet': 'Covid modelling cannot accurately predict numbers, admits Prof Graham Medley, of the Scientific Pandemic Influenza Group on Modelling (Spi-M). I think what he means is their models are rubbish and pointless. Is it time to disband SAGE? ', 'tweet_url': 'https://twitter.com/TonyHinton2016/status/1499398758462726148'}\n",
      "25 {'sentiment_label': 'neutral/negative', 'text_of_tweet': 'READ: An Epidemic Of Cardiac Arrests. The official line is that the weekly reports of people collapsing in the audience of football matches are no different from normal, only in the past the matches would not have been stopped. ', 'tweet_url': 'https://twitter.com/PanData19/status/1499328455858663437'}\n",
      "26 {'sentiment_label': 'positive', 'text_of_tweet': 'An opinion piece by Australian writer Alexandra Marshall on how the media has connived to keep their readers in the dark and in doing so, build into The Narrative.', 'tweet_url': 'https://twitter.com/PanData19/status/1498625435445383172'}\n",
      "27 {'sentiment_label': 'neutral/negative', 'text_of_tweet': \"It's precisely because masks are useless that people married to the Covid narrative will insist they remain. When you introduce a meaningless - but symbolic - policy, there's no metric or signal to remove it. Masks never worked, so they can never stop working.\", 'tweet_url': 'https://twitter.com/annbauerwriter/status/1491791526753497094'}\n",
      "28 {'sentiment_label': 'negative', 'text_of_tweet': 'COVID-19 is currently ensnarled in the most consequential culture war of our times. At the heart of this social flashpoint is a thorny question on the nature of freedom: what it is, who it belongs to and how to preserve it.', 'tweet_url': 'https://twitter.com/PanData19/status/1496566244409061383'}\n",
      "29 {'sentiment_label': 'negative', 'text_of_tweet': 'The Canadian government complained that the truckers disrupted businesses, and now the government is now closing businesses again.', 'tweet_url': 'https://twitter.com/MartinKulldorff/status/1495614865020436484'}\n",
      "30 {'sentiment_label': 'negative', 'text_of_tweet': 'Some basic maths re children and covid: If 0.0013% die with covid when infected then out of 76,923 infected kids, one will die. If you need to vaccinate 200 kids to prevent one infection then you need to vaccinate 200*76,923 = 15,384,615 to prevent one covid death. Omicron...', 'tweet_url': 'https://twitter.com/ClareCraigPath/status/1495017735469871114'}\n",
      "31 {'sentiment_label': 'neutral', 'text_of_tweet': 'Modeling is forecasting for midwits.', 'tweet_url': 'https://twitter.com/naval/status/1492388383255154692'}\n",
      "32 {'sentiment_label': 'positive', 'text_of_tweet': 'Kris you are an inspiration, like you said us traders are pieces of shit and don’t really add anything to society, but doing this we can atleast take part on something we believe in and help others', 'tweet_url': 'https://twitter.com/RyanBarredo1/status/1500171516356534284'}\n",
      "33 {'sentiment_label': 'neutral', 'text_of_tweet': \"The top for most US stocks was 1 year ago, why are you writing this stuff now (don't answer lol) Good luck\", 'tweet_url': 'https://twitter.com/Qullamaggie/status/1499455382237978632'}\n",
      "34 {'sentiment_label': 'neutral/positive', 'text_of_tweet': 'Going on adventures in Italy next few days!', 'tweet_url': 'https://twitter.com/Qullamaggie/status/1496821656513359873'}\n",
      "35 {'sentiment_label': 'neutral', 'text_of_tweet': 'Sir, what are your thoughts about the macroeconomical and geopolitical situation relative to the planetary movements and the underlying technicals in the market????', 'tweet_url': 'https://twitter.com/Qullamaggie/status/1496033687749672961'}\n",
      "36 {'sentiment_label': 'neutral', 'text_of_tweet': 'Entered TQQQ this morning with a wide stop. Would like to add to my position once we get confirmation.', 'tweet_url': 'https://twitter.com/geovanycapital/status/1497230865105141763'}\n",
      "37 {'sentiment_label': 'negative', 'text_of_tweet': \"I guess a scam artist decided to make a fake twitter account and impersonate me. I've already blocked the fake account, but please block as well to eliminate confusion: \", 'tweet_url': 'https://twitter.com/geovanycapital/status/1483916870352715777'}\n",
      "38 {'sentiment_label': 'positive', 'text_of_tweet': 'This is a great time to build a LEADER list. Through your own observation, identify which stocks you think are leaders, & track them for entries. Leaders are stocks that hold up the best during a pullback / correction, and often break out of their ranges prior to the indices.', 'tweet_url': 'https://twitter.com/geovanycapital/status/1481310881929695245'}\n",
      "39 {'sentiment_label': 'neutral', 'text_of_tweet': \"Two things I'm thinking about right now, this is as cut and dry as I can put it (1) I am looking to be patient in basic materials / energy sector for certain names to flag out and provide me an entry to take a trade (2) Searching for potential leaders to enter after we bottom\", 'tweet_url': 'https://twitter.com/geovanycapital/status/1479542115658973184'}\n",
      "40 {'sentiment_label': 'negative', 'text_of_tweet': 'Tough question, prob would have to call a friend. Flat planet friends', 'tweet_url': 'https://twitter.com/Afghanrebel/status/1496072494817718275'}\n",
      "41 {'sentiment_label': 'positive', 'text_of_tweet': 'Awesome!', 'tweet_url': 'https://twitter.com/Qullamaggie/status/1494275070214217730'}\n",
      "42 {'sentiment_label': 'positive', 'text_of_tweet': ' giving away free Bitcoin and secret on how to double your account in 2 weeks!', 'tweet_url': 'https://twitter.com/Qullamaggie/status/1493216277321723910'}\n",
      "43 {'sentiment_label': 'neutral/negative', 'text_of_tweet': 'Many traders are scan collectors. they just want scans. They never spend time understanding what the scan is doing or why it is built the way it is. Just mindlessly send DM “can you send me scan”', 'tweet_url': 'https://twitter.com/PradeepBonde/status/1492490592105996288'}\n",
      "44 {'sentiment_label': 'positive', 'text_of_tweet': 'Have a great weekend cult!', 'tweet_url': 'https://twitter.com/Qullamaggie/status/1492242776322453509'}\n",
      "45 {'sentiment_label': 'positive', 'text_of_tweet': \"I'm beginning to think the cat is in charge\", 'tweet_url': 'https://twitter.com/DarvasTrading/status/1492244456187301889'}\n",
      "46 {'sentiment_label': 'positive', 'text_of_tweet': 'I know I use this platform for trading, but yesterday was one of the best days of my life ', 'tweet_url': 'https://twitter.com/geovanycapital/status/1473116249215303688'}\n",
      "47 {'sentiment_label': 'positive', 'text_of_tweet': 'CONGRATULATIONS MAN!!! Super happy for you both', 'tweet_url': 'https://twitter.com/geovanycapital/status/1473116249215303688'}\n",
      "48 {'sentiment_label': 'positive', 'text_of_tweet': 'Achievement unlocked', 'tweet_url': 'https://twitter.com/skittlekillerx/status/1500313266924527618'}\n",
      "49 {'sentiment_label': 'positive', 'text_of_tweet': \"Have I mentioned I'm in engaged?! Today marks exactly one month since I said yes and I couldn't be more excited!\", 'tweet_url': 'https://twitter.com/JourneyWJBlog/status/1500207018623348736'}\n",
      "50 {'sentiment_label': 'positive', 'text_of_tweet': \"A typical day. As usual, Louie's highly tuned senses are alert and ready for any eventuality. \", 'tweet_url': 'https://twitter.com/lynda_nicholls/status/1500477373837418507'}\n"
     ]
    }
   ],
   "source": [
    "for id_, tweet_info in my_tweets.items():\n",
    "    print(id_, tweet_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5 points] Question 3:\n",
    "\n",
    "Run VADER on your own tweets (see function **run_vader** from notebook **Lab2-Sentiment-analysis-using-VADER.ipynb**). You can use the code snippet below this explanation as a starting point. \n",
    "* [2.5 points] a. Perform a quantitative evaluation. Explain the different scores, and explain which scores are most relevant and why.\n",
    "* [2.5 points] b. Perform an error analysis: select 10 positive, 10 negative and 10 neutral tweets that are not correctly classified and try to understand why. Refer to the VADER-rules and the VADER-lexicon. Of course, if there are less than 10 errors for a category, you only have to check those. For example, if there are only 5 errors for positive tweets, you just describe those."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2.5 points] a. Most relevant scores are the ones from tweets 7, 10, 14, 16, 18, 20, 21, 22, 23, 24, 26, 30, 33, 36, 40, 45, 48. This scores are the most relevant since they give different outcomes than the ones predicted in the my_tweets.json. Most of them are classified as possitive or neutral by the vader output label, when the tone of the tweets it's negative.\n",
    "\n",
    "[2.5 points] b.\n",
    "\n",
    "Positive tweets: 10,23,26,45,48\n",
    "\n",
    "tweet 10: \"ata from the #Sentinel5P satellite are being used to detect emission hotspots in various regions, identifying #methane concentrations and locating exact facilities leaking methane\". (token as neutral)\n",
    "\n",
    "tweet 23: \"Excellent discussion covering the intersections between COVID-19 and geopolitical crisis with Professor Isa Blumi and Jesse Zurawell. Essential listening. NOTE: It's the 4th interview down on the list, dated 1 March 2022\"  (token as negative)\n",
    "\n",
    "tweet 26: \"An opinion piece by Australian writer Alexandra Marshall on how the media has connived to keep their readers in the dark and in doing so, build into The Narrative.\"  (token as neutral)\n",
    "\n",
    "tweet 45: \"I'm beginning to think the cat is in charge\"  (token as neutral)\n",
    "\n",
    "tweet 48: \"Achievement unlocked\" (token as neutral)\n",
    "\n",
    "The following positive sentences have these errors because not all parts of speech are provided, also positive or affirmative keyboards are missing. Positive tags should be added to the list of affirmation tokens, so it should appropriately flip the sentiment polarity while maintaining the intensity calculations. Also it is important to avoid special characters for a better predicitions. \n",
    "\n",
    "\n",
    "\n",
    "Negative tweets: 7,20,21,22,24,30,40\n",
    "\n",
    "tweet 7: \"This man drove to Poland from DENMARK to help Ukrainian women and children. He said he is going back with six people, whom he will help settle in a new country, over 2000 km. away from home. “Putin is a dictator, and we don’t like that in Denmark,” he said.\" (token as positive)\n",
    "\n",
    "tweet 20:\"Ran for a train today for the first time ever. My mcm better tweet “if they wanted, to they would”\"(token as positive)\n",
    "\n",
    "tweet 21: \"My mistery: hyperglycemia, not yet diabete, low insulin level, low pep-c level, no D1 antibody, no mody, pancreas ok…no doctor has an explanation…any idea? Thanks\" (token as positive)\n",
    "\n",
    "tweet 22: \"Events of the last two years highlight the extreme danger presented by centralization. The intellectual backing for the idea that we should have more of it rests on the notion that having more data enables better centralized decision-making. /1\" (token as positive)\n",
    "\n",
    "tweet 24: \"Covid modelling cannot accurately predict numbers, admits Prof Graham Medley, of the Scientific Pandemic Influenza Group on Modelling (Spi-M). I think what he means is their models are rubbish and pointless. Is it time to disband SAGE? \" (token as positive)\n",
    "\n",
    "tweet 40: \"Tough question, prob would have to call a friend. Flat planet friends\" (token as positive)\n",
    "\n",
    "The following negative sentences have these errors because not all parts of speech are provided, also negation keyboards are missing. Negative tags such as {not} ,{bad}, etc should be added to the list of negation tokens, so it should appropriately flip the sentiment polarity while maintaining the intensity calculations. Also it is important to avoid special characters for a better predicitions. \n",
    "\n",
    "\n",
    "\n",
    "Neutral tweets: 14,16,18,20,22,33,36\n",
    "\n",
    "tweet 14: \"Deciding between security cameras or video doorbells can be tough. We cover the pros and cons of installation, video coverage, and price.\" (token as positive)\n",
    "\n",
    "\n",
    "tweet 16: \"The James Carey Urban Communication Grant supports research that advances knowledge on the consequences of urban communication in and across urban societies. The grant is awarded by the Urban Communication Foundation (UCF) and co-sponsored\"(token as positive)\n",
    "\n",
    "\n",
    "tweet 18: \"“What’s the trend of the market? If it’s negative, you’ll want to do very little, if any buying, even if you see some stocks breaking out. Your probabilities of success are quite low when the market trend is going against you.”Stan Weinstein\"(token as negative)\n",
    "\n",
    "\n",
    "tweet 20: \"Ran for a train today for the first time ever. My mcm better tweet “if they wanted, to they would”\"(token as positive)\n",
    "\n",
    "\n",
    "tweet 22: \"Events of the last two years highlight the extreme danger presented by centralization. The intellectual backing for the idea that we should have more of it rests on the notion that having more data enables better centralized decision-making. /1\"(token as positive)\n",
    "\n",
    "\n",
    "tweet 33: \"The top for most US stocks was 1 year ago, why are you writing this stuff now (don't answer lol) Good luck\"(token as positive)\n",
    "\n",
    "\n",
    "tweet 36: \"Entered TQQQ this morning with a wide stop. Would like to add to my position once we get confirmation.\"(token as positive)\n",
    "\n",
    "\n",
    "\n",
    "The following neutral sentences have these errors because not all parts of speech are provided, also neutral keyboards are missing. Neutral tags should be added to the list of neutral tokens, so it should appropriately flip the sentiment polarity while maintaining the intensity calculations. Also it is important to avoid special characters for a better predicitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vader_output_to_label(vader_output):\n",
    "    \"\"\"\n",
    "    map vader output e.g.,\n",
    "    {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.4215}\n",
    "    to one of the following values:\n",
    "    a) positive float -> 'positive'\n",
    "    b) 0.0 -> 'neutral'\n",
    "    c) negative float -> 'negative'\n",
    "    \n",
    "    :param dict vader_output: output dict from vader\n",
    "    \n",
    "    :rtype: str\n",
    "    :return: 'negative' | 'neutral' | 'positive'\n",
    "    \"\"\"\n",
    "    compound = vader_output['compound']\n",
    "    \n",
    "    if compound < 0:\n",
    "        return 'negative'\n",
    "    elif compound == 0.0:\n",
    "        return 'neutral'\n",
    "    elif compound > 0.0:\n",
    "        return 'positive'\n",
    "    \n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.0}) == 'neutral'\n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.01}) == 'positive'\n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': -0.01}) == 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 VADER OUTPUT {'neg': 0.346, 'neu': 0.654, 'pos': 0.0, 'compound': -0.489} negative\n",
      "CORRECT SENTIMENT negative\n",
      "2 VADER OUTPUT {'neg': 0.0, 'neu': 0.909, 'pos': 0.091, 'compound': 0.3612} positive\n",
      "CORRECT SENTIMENT positive\n",
      "3 VADER OUTPUT {'neg': 0.175, 'neu': 0.825, 'pos': 0.0, 'compound': -0.8271} negative\n",
      "CORRECT SENTIMENT negative\n",
      "4 VADER OUTPUT {'neg': 0.062, 'neu': 0.905, 'pos': 0.032, 'compound': -0.34} negative\n",
      "CORRECT SENTIMENT negative\n",
      "5 VADER OUTPUT {'neg': 0.313, 'neu': 0.612, 'pos': 0.075, 'compound': -0.5423} negative\n",
      "CORRECT SENTIMENT negative\n",
      "6 VADER OUTPUT {'neg': 0.091, 'neu': 0.909, 'pos': 0.0, 'compound': -0.296} negative\n",
      "CORRECT SENTIMENT negative\n",
      "7 VADER OUTPUT {'neg': 0.0, 'neu': 0.851, 'pos': 0.149, 'compound': 0.7845} positive\n",
      "CORRECT SENTIMENT negative\n",
      "8 VADER OUTPUT {'neg': 0.118, 'neu': 0.796, 'pos': 0.086, 'compound': -0.2648} negative\n",
      "CORRECT SENTIMENT neutral\n",
      "9 VADER OUTPUT {'neg': 0.0, 'neu': 0.815, 'pos': 0.185, 'compound': 0.7996} positive\n",
      "CORRECT SENTIMENT positive\n",
      "10 VADER OUTPUT {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} neutral\n",
      "CORRECT SENTIMENT positive\n",
      "11 VADER OUTPUT {'neg': 0.0, 'neu': 0.912, 'pos': 0.088, 'compound': 0.4939} positive\n",
      "CORRECT SENTIMENT positive\n",
      "12 VADER OUTPUT {'neg': 0.107, 'neu': 0.893, 'pos': 0.0, 'compound': -0.4939} negative\n",
      "CORRECT SENTIMENT negative\n",
      "13 VADER OUTPUT {'neg': 0.11, 'neu': 0.89, 'pos': 0.0, 'compound': -0.4939} negative\n",
      "CORRECT SENTIMENT negative\n",
      "14 VADER OUTPUT {'neg': 0.063, 'neu': 0.837, 'pos': 0.1, 'compound': 0.2263} positive\n",
      "CORRECT SENTIMENT neutral\n",
      "15 VADER OUTPUT {'neg': 0.0, 'neu': 0.923, 'pos': 0.077, 'compound': 0.3612} positive\n",
      "CORRECT SENTIMENT positive\n",
      "16 VADER OUTPUT {'neg': 0.0, 'neu': 0.746, 'pos': 0.254, 'compound': 0.8481} positive\n",
      "CORRECT SENTIMENT neutral\n",
      "17 VADER OUTPUT {'neg': 0.17, 'neu': 0.74, 'pos': 0.09, 'compound': -0.3597} negative\n",
      "CORRECT SENTIMENT negative\n",
      "18 VADER OUTPUT {'neg': 0.124, 'neu': 0.774, 'pos': 0.102, 'compound': -0.2716} negative\n",
      "CORRECT SENTIMENT neutral\n",
      "19 VADER OUTPUT {'neg': 0.087, 'neu': 0.913, 'pos': 0.0, 'compound': -0.2732} negative\n",
      "CORRECT SENTIMENT neutral/negative\n",
      "20 VADER OUTPUT {'neg': 0.0, 'neu': 0.861, 'pos': 0.139, 'compound': 0.4404} positive\n",
      "CORRECT SENTIMENT negative/neutral\n",
      "21 VADER OUTPUT {'neg': 0.142, 'neu': 0.647, 'pos': 0.211, 'compound': 0.2796} positive\n",
      "CORRECT SENTIMENT negative\n",
      "22 VADER OUTPUT {'neg': 0.072, 'neu': 0.718, 'pos': 0.21, 'compound': 0.6771} positive\n",
      "CORRECT SENTIMENT neutral/negative\n",
      "23 VADER OUTPUT {'neg': 0.111, 'neu': 0.788, 'pos': 0.101, 'compound': -0.1027} negative\n",
      "CORRECT SENTIMENT positive\n",
      "24 VADER OUTPUT {'neg': 0.0, 'neu': 0.941, 'pos': 0.059, 'compound': 0.296} positive\n",
      "CORRECT SENTIMENT negative\n",
      "25 VADER OUTPUT {'neg': 0.166, 'neu': 0.796, 'pos': 0.038, 'compound': -0.6842} negative\n",
      "CORRECT SENTIMENT neutral/negative\n",
      "26 VADER OUTPUT {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} neutral\n",
      "CORRECT SENTIMENT positive\n",
      "27 VADER OUTPUT {'neg': 0.145, 'neu': 0.805, 'pos': 0.051, 'compound': -0.5136} negative\n",
      "CORRECT SENTIMENT neutral/negative\n",
      "28 VADER OUTPUT {'neg': 0.132, 'neu': 0.78, 'pos': 0.088, 'compound': -0.2648} negative\n",
      "CORRECT SENTIMENT negative\n",
      "29 VADER OUTPUT {'neg': 0.137, 'neu': 0.863, 'pos': 0.0, 'compound': -0.4019} negative\n",
      "CORRECT SENTIMENT negative\n",
      "30 VADER OUTPUT {'neg': 0.3, 'neu': 0.663, 'pos': 0.036, 'compound': -0.9578} negative\n",
      "CORRECT SENTIMENT negative\n",
      "31 VADER OUTPUT {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} neutral\n",
      "CORRECT SENTIMENT neutral\n",
      "32 VADER OUTPUT {'neg': 0.054, 'neu': 0.771, 'pos': 0.175, 'compound': 0.6369} positive\n",
      "CORRECT SENTIMENT positive\n",
      "33 VADER OUTPUT {'neg': 0.095, 'neu': 0.714, 'pos': 0.19, 'compound': 0.3387} positive\n",
      "CORRECT SENTIMENT neutral\n",
      "34 VADER OUTPUT {'neg': 0.0, 'neu': 0.722, 'pos': 0.278, 'compound': 0.4003} positive\n",
      "CORRECT SENTIMENT neutral/positive\n",
      "35 VADER OUTPUT {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} neutral\n",
      "CORRECT SENTIMENT neutral\n",
      "36 VADER OUTPUT {'neg': 0.106, 'neu': 0.773, 'pos': 0.121, 'compound': 0.0772} positive\n",
      "CORRECT SENTIMENT neutral\n",
      "37 VADER OUTPUT {'neg': 0.383, 'neu': 0.471, 'pos': 0.146, 'compound': -0.7935} negative\n",
      "CORRECT SENTIMENT negative\n",
      "38 VADER OUTPUT {'neg': 0.0, 'neu': 0.838, 'pos': 0.162, 'compound': 0.8519} positive\n",
      "CORRECT SENTIMENT positive\n",
      "39 VADER OUTPUT {'neg': 0.038, 'neu': 0.886, 'pos': 0.076, 'compound': 0.2732} positive\n",
      "CORRECT SENTIMENT neutral\n",
      "40 VADER OUTPUT {'neg': 0.095, 'neu': 0.506, 'pos': 0.399, 'compound': 0.7003} positive\n",
      "CORRECT SENTIMENT negative\n",
      "41 VADER OUTPUT {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.6588} positive\n",
      "CORRECT SENTIMENT positive\n",
      "42 VADER OUTPUT {'neg': 0.0, 'neu': 0.667, 'pos': 0.333, 'compound': 0.7177} positive\n",
      "CORRECT SENTIMENT positive\n",
      "43 VADER OUTPUT {'neg': 0.0, 'neu': 0.965, 'pos': 0.035, 'compound': 0.0772} positive\n",
      "CORRECT SENTIMENT neutral/negative\n",
      "44 VADER OUTPUT {'neg': 0.0, 'neu': 0.406, 'pos': 0.594, 'compound': 0.6588} positive\n",
      "CORRECT SENTIMENT positive\n",
      "45 VADER OUTPUT {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} neutral\n",
      "CORRECT SENTIMENT positive\n",
      "46 VADER OUTPUT {'neg': 0.0, 'neu': 0.734, 'pos': 0.266, 'compound': 0.7783} positive\n",
      "CORRECT SENTIMENT positive\n",
      "47 VADER OUTPUT {'neg': 0.0, 'neu': 0.234, 'pos': 0.766, 'compound': 0.9338} positive\n",
      "CORRECT SENTIMENT positive\n",
      "48 VADER OUTPUT {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} neutral\n",
      "CORRECT SENTIMENT positive\n",
      "49 VADER OUTPUT {'neg': 0.105, 'neu': 0.743, 'pos': 0.152, 'compound': 0.2573} positive\n",
      "CORRECT SENTIMENT positive\n",
      "50 VADER OUTPUT {'neg': 0.0, 'neu': 0.734, 'pos': 0.266, 'compound': 0.5719} positive\n",
      "CORRECT SENTIMENT positive\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import vader\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import pathlib\n",
    "import sklearn\n",
    "import numpy\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "tweets = []\n",
    "all_vader_output = []\n",
    "gold = []\n",
    "\n",
    "# settings (to change for different experiments)\n",
    "to_lemmatize = True \n",
    "pos = set()\n",
    "\n",
    "for id_, tweet_info in my_tweets.items():\n",
    "    the_tweet = tweet_info['text_of_tweet']\n",
    "    vader_output = SentimentIntensityAnalyzer() # run vader\n",
    "    vader_label = vader_output.polarity_scores(the_tweet)# convert vader output to category\n",
    "    \n",
    "    tweets.append(the_tweet)\n",
    "    all_vader_output.append(vader_label)\n",
    "    gold.append(tweet_info['sentiment_label'])\n",
    "    \n",
    "    print(id_,'VADER OUTPUT', vader_label, vader_output_to_label(vader_label))\n",
    "    print('CORRECT SENTIMENT', tweet_info['sentiment_label'])\n",
    "\n",
    "# use scikit-learn's classification report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [4 points] Question 4:\n",
    "Run VADER on the set of airline tweets with the following settings:\n",
    "\n",
    "* Run VADER (as it is) on the set of airline tweets \n",
    "* Run VADER on the set of airline tweets after having lemmatized the text\n",
    "* Run VADER on the set of airline tweets with only adjectDecisionTreeClassifiere set of airline tweets with only adjectives and after having lemmatized the text\n",
    "* Run VADER on the set of airline tweets with only nouns\n",
    "* Run VADER on the set of airline tweets with only nouns and after having lemmatized the text\n",
    "* Run VADER on the set of airline tweets with only verbs\n",
    "* Run VADER on the set of airline tweets with only verbs and after having lemmatized the text\n",
    "\n",
    "* [1 point] a. Generate for all separate experiments the classification report, i.e., Precision, Recall, and F<sub>1</sub> scores per category as well as micro and macro averages. **Use a different code cell (or multiple code cells) for each experiment.**\n",
    "* [3 points] b. Compare the scores and explain what they tell you.\n",
    "* - Does lemmatisation help? Explain why or why not.\n",
    "Lemmatisation checks for the dictionary meaning when changing into base-form, which is usefull when a word has a meaning that is important to decide if a sentence is positive, negative or neutral, therefore it increases the accuracy of the sentiment score.\n",
    "* - Are all parts of speech equally important for sentiment analysis? Explain why or why not.\n",
    "We would say that are all part os speech are equally important for sentiment analysis because based on the parts of speech,whether the noun or verb of a word for example is meant, the sentiment can differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "cwd = pathlib.Path.cwd()\n",
    "airline_tweets_folder = cwd.joinpath('airlinetweets')\n",
    "airline_tweets = load_files('C:/Users/zakar/Downloads/ba-text-mining-master/ba-text-mining-master/lab_sessions/lab3/airlinetweets/airlinetweets')\n",
    "all_tweets = airline_tweets.data\n",
    "vader_model = SentimentIntensityAnalyzer()\n",
    "gold_labels = airline_tweets.target\n",
    "\n",
    "def run_vader(textual_unit, \n",
    "              lemmatize=False, \n",
    "              parts_of_speech_to_consider=None,\n",
    "              verbose=0):\n",
    "    \"\"\"\n",
    "    Run VADER on a sentence from spacy\n",
    "    \n",
    "    :param str textual unit: a textual unit, e.g., sentence, sentences (one string)\n",
    "    (by looping over doc.sents)\n",
    "    :param bool lemmatize: If True, provide lemmas to VADER instead of words\n",
    "    :param set parts_of_speech_to_consider:\n",
    "    -None or empty set: all parts of speech are provided\n",
    "    -non-empty set: only these parts of speech are considered.\n",
    "    :param int verbose: if set to 1, information is printed\n",
    "    about input and output\n",
    "    \n",
    "    :rtype: dict\n",
    "    :return: vader output dict\n",
    "    \"\"\"\n",
    "    doc = nlp(textual_unit)\n",
    "        \n",
    "    input_to_vader = []\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "\n",
    "            to_add = token.text\n",
    "\n",
    "            if lemmatize:\n",
    "                to_add = token.lemma_\n",
    "\n",
    "                if to_add == '-PRON-': \n",
    "                    to_add = token.text\n",
    "\n",
    "            if parts_of_speech_to_consider:\n",
    "                if token.pos_ in parts_of_speech_to_consider:\n",
    "                    input_to_vader.append(to_add) \n",
    "            else:\n",
    "                input_to_vader.append(to_add)\n",
    "\n",
    "    scores = vader_model.polarity_scores(' '.join(input_to_vader))\n",
    "    \n",
    "    if verbose >= 1:\n",
    "        print()\n",
    "        print('INPUT SENTENCE', sent)\n",
    "        print('INPUT TO VADER', input_to_vader)\n",
    "        print('VADER OUTPUT', scores)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mix of label input types (string and number)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5172/2461490258.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mthree_class_system_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvader_output_to_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvader_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mreport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgold_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mthree_class_system_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdigits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreport\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[1;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[0;32m   1971\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1972\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1973\u001b[1;33m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1974\u001b[0m         \u001b[0mlabels_given\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1975\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\multiclass.py\u001b[0m in \u001b[0;36munique_labels\u001b[1;34m(*ys)\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;31m# Check that we don't mix string type with number type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mys_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Mix of label input types (string and number)\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mys_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Mix of label input types (string and number)"
     ]
    }
   ],
   "source": [
    "#VADER (as it is) on the set of airline tweets\n",
    "three_class_system_output = []\n",
    "for i in range(0,len(all_tweets)):\n",
    "    vader_scores = run_vader(str(all_tweets[i]), \n",
    "                    lemmatize = False,\n",
    "                    verbose = 0)\n",
    "    three_class_system_output.append(vader_output_to_label(vader_scores))\n",
    "\n",
    "report = classification_report(gold_labels,three_class_system_output,digits = 3)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E866] Expected a string or 'Doc' as input, but got: <class 'bytes'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4416/1345132750.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#VADER on the set of airline tweets after having lemmatized the text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_tweets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     print(run_vader(all_tweets[i],\n\u001b[0m\u001b[0;32m      4\u001b[0m                     \u001b[0mlemmatize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                     verbose = 1))\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4416/1461549522.py\u001b[0m in \u001b[0;36mrun_vader\u001b[1;34m(textual_unit, lemmatize, parts_of_speech_to_consider, verbose)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mvader\u001b[0m \u001b[0moutput\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \"\"\"\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtextual_unit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0minput_to_vader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m   1003\u001b[0m         \u001b[0mDOCS\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mhttps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mapi\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;31m#call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1004\u001b[0m         \"\"\"\n\u001b[1;32m-> 1005\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1006\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcomponent_cfg\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1007\u001b[0m             \u001b[0mcomponent_cfg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m_ensure_doc\u001b[1;34m(self, doc_like)\u001b[0m\n\u001b[0;32m   1094\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1095\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1096\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE866\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1097\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_ensure_doc_with_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc_like\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDoc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mDoc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: [E866] Expected a string or 'Doc' as input, but got: <class 'bytes'>."
     ]
    }
   ],
   "source": [
    "#VADER on the set of airline tweets after having lemmatized the text\n",
    "for i in range(0,len(all_tweets)):\n",
    "    print(run_vader(str(all_tweets[i]),\n",
    "                    lemmatize = True,\n",
    "                    verbose = 1))\n",
    "    three_class_human_annotation = tweet_info['sentiment_label']\n",
    "    three_class_system_output = vader_output_to_label(vader_label)\n",
    "    report = classification_report(three_class_human_annotation,three_class_system_output,digits = 3)\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E866] Expected a string or 'Doc' as input, but got: <class 'bytes'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8080/4043412656.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#VADER on the set of airline tweets with only adjectives\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_tweets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     print(run_vader(all_tweets[i], \n\u001b[0m\u001b[0;32m      4\u001b[0m           \u001b[0mlemmatize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m           \u001b[0mparts_of_speech_to_consider\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'ADJ'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8080/1461549522.py\u001b[0m in \u001b[0;36mrun_vader\u001b[1;34m(textual_unit, lemmatize, parts_of_speech_to_consider, verbose)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mvader\u001b[0m \u001b[0moutput\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \"\"\"\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtextual_unit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0minput_to_vader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m   1003\u001b[0m         \u001b[0mDOCS\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mhttps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mapi\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;31m#call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1004\u001b[0m         \"\"\"\n\u001b[1;32m-> 1005\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1006\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcomponent_cfg\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1007\u001b[0m             \u001b[0mcomponent_cfg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m_ensure_doc\u001b[1;34m(self, doc_like)\u001b[0m\n\u001b[0;32m   1094\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1095\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1096\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE866\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1097\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_ensure_doc_with_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc_like\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDoc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mDoc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: [E866] Expected a string or 'Doc' as input, but got: <class 'bytes'>."
     ]
    }
   ],
   "source": [
    "#VADER on the set of airline tweets with only adjectives\n",
    "for i in range(0,len(all_tweets)):\n",
    "    print(run_vader(str(all_tweets[i]), \n",
    "          lemmatize=False, \n",
    "          parts_of_speech_to_consider={'ADJ'},\n",
    "          verbose=1))\n",
    "    three_class_human_annotation = tweet_info['sentiment_label']\n",
    "    three_class_system_output = vader_output_to_label(vader_label)\n",
    "    report = classification_report(three_class_human_annotation,three_class_system_output,digits = 3)\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E866] Expected a string or 'Doc' as input, but got: <class 'bytes'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8080/3187759074.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#VADER on the set of airline tweets with only adjectives and after having lemmatized the text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_tweets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     print(run_vader(all_tweets[i], \n\u001b[0m\u001b[0;32m      4\u001b[0m           \u001b[0mlemmatize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m           \u001b[0mparts_of_speech_to_consider\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'ADJ'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8080/1461549522.py\u001b[0m in \u001b[0;36mrun_vader\u001b[1;34m(textual_unit, lemmatize, parts_of_speech_to_consider, verbose)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mvader\u001b[0m \u001b[0moutput\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \"\"\"\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtextual_unit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0minput_to_vader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m   1003\u001b[0m         \u001b[0mDOCS\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mhttps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mapi\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;31m#call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1004\u001b[0m         \"\"\"\n\u001b[1;32m-> 1005\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1006\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcomponent_cfg\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1007\u001b[0m             \u001b[0mcomponent_cfg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m_ensure_doc\u001b[1;34m(self, doc_like)\u001b[0m\n\u001b[0;32m   1094\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1095\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1096\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE866\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1097\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_ensure_doc_with_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc_like\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDoc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mDoc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: [E866] Expected a string or 'Doc' as input, but got: <class 'bytes'>."
     ]
    }
   ],
   "source": [
    "#VADER on the set of airline tweets with only adjectives and after having lemmatized the text\n",
    "for i in range(0,len(all_tweets)):\n",
    "    print(run_vader(str(all_tweets[i]), \n",
    "          lemmatize=True, \n",
    "          parts_of_speech_to_consider={'ADJ'},\n",
    "          verbose=1))\n",
    "    three_class_human_annotation = tweet_info['sentiment_label']\n",
    "    three_class_system_output = vader_output_to_label(vader_label)\n",
    "    report = classification_report(three_class_human_annotation,three_class_system_output,digits = 3)\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E866] Expected a string or 'Doc' as input, but got: <class 'bytes'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8080/3873697405.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#VADER on the set of airline tweets with only nouns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_tweets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     print(run_vader(all_tweets[i], \n\u001b[0m\u001b[0;32m      4\u001b[0m           \u001b[0mlemmatize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m           \u001b[0mparts_of_speech_to_consider\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'NOUN'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8080/1461549522.py\u001b[0m in \u001b[0;36mrun_vader\u001b[1;34m(textual_unit, lemmatize, parts_of_speech_to_consider, verbose)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mvader\u001b[0m \u001b[0moutput\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \"\"\"\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtextual_unit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0minput_to_vader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m   1003\u001b[0m         \u001b[0mDOCS\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mhttps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mapi\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;31m#call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1004\u001b[0m         \"\"\"\n\u001b[1;32m-> 1005\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1006\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcomponent_cfg\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1007\u001b[0m             \u001b[0mcomponent_cfg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m_ensure_doc\u001b[1;34m(self, doc_like)\u001b[0m\n\u001b[0;32m   1094\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1095\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1096\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE866\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1097\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_ensure_doc_with_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc_like\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDoc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mDoc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: [E866] Expected a string or 'Doc' as input, but got: <class 'bytes'>."
     ]
    }
   ],
   "source": [
    "#VADER on the set of airline tweets with only nouns\n",
    "for i in range(0,len(all_tweets)):\n",
    "    print(run_vader(str(all_tweets[i]), \n",
    "          lemmatize=False, \n",
    "          parts_of_speech_to_consider={'NOUN'},\n",
    "          verbose=1))\n",
    "    three_class_human_annotation = tweet_info['sentiment_label']\n",
    "    three_class_system_output = vader_output_to_label(vader_label)\n",
    "    report = classification_report(three_class_human_annotation,three_class_system_output,digits = 3)\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E866] Expected a string or 'Doc' as input, but got: <class 'bytes'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8080/1050713995.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#VADER on the set of airline tweets with only nouns and after having lemmatized the text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_tweets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     print(run_vader(all_tweets[i], \n\u001b[0m\u001b[0;32m      4\u001b[0m           \u001b[0mlemmatize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m           \u001b[0mparts_of_speech_to_consider\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'NOUN'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8080/1461549522.py\u001b[0m in \u001b[0;36mrun_vader\u001b[1;34m(textual_unit, lemmatize, parts_of_speech_to_consider, verbose)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mvader\u001b[0m \u001b[0moutput\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \"\"\"\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtextual_unit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0minput_to_vader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m   1003\u001b[0m         \u001b[0mDOCS\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mhttps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mapi\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;31m#call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1004\u001b[0m         \"\"\"\n\u001b[1;32m-> 1005\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1006\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcomponent_cfg\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1007\u001b[0m             \u001b[0mcomponent_cfg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m_ensure_doc\u001b[1;34m(self, doc_like)\u001b[0m\n\u001b[0;32m   1094\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1095\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1096\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE866\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1097\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_ensure_doc_with_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc_like\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDoc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mDoc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: [E866] Expected a string or 'Doc' as input, but got: <class 'bytes'>."
     ]
    }
   ],
   "source": [
    "#VADER on the set of airline tweets with only nouns and after having lemmatized the text\n",
    "for i in range(0,len(all_tweets)):\n",
    "    print(run_vader(str(all_tweets[i]), \n",
    "          lemmatize=True, \n",
    "          parts_of_speech_to_consider={'NOUN'},\n",
    "          verbose=1))\n",
    "    three_class_human_annotation = tweet_info['sentiment_label']\n",
    "    three_class_system_output = vader_output_to_label(vader_label)\n",
    "    report = classification_report(three_class_human_annotation,three_class_system_output,digits = 3)\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E866] Expected a string or 'Doc' as input, but got: <class 'bytes'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8080/1316896009.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#VADER on the set of airline tweets with only verbs and after having lemmatized the text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_tweets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     print(run_vader(all_tweets[i], \n\u001b[0m\u001b[0;32m      4\u001b[0m           \u001b[0mlemmatize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m           \u001b[0mparts_of_speech_to_consider\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'VERB'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8080/1461549522.py\u001b[0m in \u001b[0;36mrun_vader\u001b[1;34m(textual_unit, lemmatize, parts_of_speech_to_consider, verbose)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mvader\u001b[0m \u001b[0moutput\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \"\"\"\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtextual_unit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0minput_to_vader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m   1003\u001b[0m         \u001b[0mDOCS\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mhttps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mapi\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;31m#call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1004\u001b[0m         \"\"\"\n\u001b[1;32m-> 1005\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1006\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcomponent_cfg\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1007\u001b[0m             \u001b[0mcomponent_cfg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m_ensure_doc\u001b[1;34m(self, doc_like)\u001b[0m\n\u001b[0;32m   1094\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1095\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1096\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE866\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1097\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_ensure_doc_with_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc_like\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDoc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mDoc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: [E866] Expected a string or 'Doc' as input, but got: <class 'bytes'>."
     ]
    }
   ],
   "source": [
    "#VADER on the set of airline tweets with only verbs and after having lemmatized the text\n",
    "for i in range(0,len(all_tweets)):\n",
    "    print(run_vader(str(all_tweets[i]), \n",
    "          lemmatize=True, \n",
    "          parts_of_speech_to_consider={'VERB'},\n",
    "          verbose=1))\n",
    "    three_class_human_annotation = tweet_info['sentiment_label']\n",
    "    three_class_system_output = vader_output_to_label(vader_label)\n",
    "    report = classification_report(three_class_human_annotation,three_class_system_output,digits = 3)\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: scikit-learn assignments\n",
    "### [4 points] Question 5\n",
    "Train the scikit-learn classifier (Naive Bayes) using the airline tweets.\n",
    "\n",
    "+ Train the model on the airline tweets with 80% training and 20% test set and default settings (TF-IDF representation, min_df=2)\n",
    "+ Train with different settings:\n",
    "    + with respect to vectorizing: TF-IDF ('airline_tfidf') vs. Bag of words representation ('airline_count') \n",
    "    + with respect to the frequency threshold (min_df). Carry out experiments with increasing values for document frequency (min_df = 2; min_df = 5; min_df =10) \n",
    "* [1 point] a. Generate a classification_report for all experiments\n",
    "* [3 points] b. Look at the results of the experiments with the different settings and try to explain why they differ: \n",
    "    + which category performs best, is this the case for any setting?\n",
    "    + does the frequency threshold affect the scores? Why or why not according to you?\n",
    "b)    \n",
    "We think that we could use similar for loops like in the previous exercise for all the experiment, but then \n",
    "run them on the airlinetweets.For the other settings we would have to change min_df from 2 to 5 and to 10 in \n",
    "CountVectorizer. We expect that setting the min_df too high(so at 10) might cause too many important terms to be ignored.\n",
    "But setting min_df too low(so at 1 or 2) might cause too many terms to be considered and that can decrease the performance.\n",
    "To conclude, we think that setting the min_df somewehere in between(so at 5) will give the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'C:\\\\Users\\\\zakar\\\\Downloads\\\\ba-text-mining-master\\\\ba-text-mining-master\\\\lab_sessions\\\\lab3\\\\airlinetweets\\\\airlinetweets\\\\positive'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15100/173779279.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mcwd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mairline_tweets_folder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcwd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoinpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'airlinetweets'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mairline_tweets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mairline_tweets_folder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mairline_counts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mairline_vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mairline_tweets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mtfidf_transformer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfTransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\datasets\\_base.py\u001b[0m in \u001b[0;36mload_files\u001b[1;34m(container_path, description, categories, load_content, shuffle, encoding, decode_error, random_state)\u001b[0m\n\u001b[0;32m    219\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 221\u001b[1;33m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mencoding\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'C:\\\\Users\\\\zakar\\\\Downloads\\\\ba-text-mining-master\\\\ba-text-mining-master\\\\lab_sessions\\\\lab3\\\\airlinetweets\\\\airlinetweets\\\\positive'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "airline_vec = CountVectorizer(min_df=2, # If a token appears fewer times than this, across all documents, it will be ignored\n",
    "                             tokenizer=nltk.word_tokenize, # we use the nltk tokenizer\n",
    "                             stop_words=stopwords.words('english')) # stopwords are removed\n",
    "\n",
    "cwd = pathlib.Path.cwd()\n",
    "airline_tweets_folder = cwd.joinpath('airlinetweets')\n",
    "airline_tweets = load_files(str(airline_tweets_folder))\n",
    "airline_counts = airline_vec.fit_transform(airline_tweets.data)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "airline_tfidf = tfidf_transformer.fit_transform(airline_counts)\n",
    "\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "    airline_counts, # the tf-idf model\n",
    "    airline_tweets.target, # the category values for each tweet \n",
    "    test_size = 0.20 # we use 80% for training and 20% for development\n",
    "    ) \n",
    "\n",
    "clf = MultinomialNB().fit(docs_train, y_train)\n",
    "y_pred = clf.predict(docs_test)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(0,len(airline_tweets.data)):\n",
    "    print('one tweet review:', airline_tweets.data[i])\n",
    "    print('gold label:', airline_tweets.target[i])\n",
    "    print('classifier predicted:', y_pred[i])\n",
    "#We keep getting an error that permission to the airlinetweets files is denied. \n",
    "#Therefore we could not experiment at all, while we do know how to carry out the experiment. \n",
    "#Read the explanation at question b.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [4 points] Question 6: Inspecting the best scoring features \n",
    "\n",
    "+ Train the scikit-learn classifier (Naive Bayes) model with the following settings (airline tweets 80% training and 20% test;  Bag of words representation ('airline_count'), min_df=2)\n",
    "* [1 point] a. Generate the list of best scoring features per class (see function **important_features_per_class** below) [1 point]\n",
    "* [3 points] b. Look at the lists and consider the following issues: \n",
    "    + [1 point] Which features did you expect for each separate class and why?\n",
    "    + [1 point] Which features did you not expect and why ? \n",
    "    + [1 point] The list contains all kinds of words such as names of airlines, punctuation, numbers and content words (e.g., 'delay' and 'bad'). Which words would you remove or keep when trying to improve the model and why? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\zakar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'C:\\\\Users\\\\zakar\\\\Downloads\\\\ba-text-mining-master\\\\ba-text-mining-master\\\\lab_sessions\\\\lab3\\\\airlinetweets\\\\airlinetweets\\\\positive'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9096/1147701571.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mairline_tweets_folder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcwd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoinpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'airlinetweets'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mairline_tweets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mairline_tweets_folder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[0mairline_counts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mairline_vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mairline_tweets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[0mtfidf_transformer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfTransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\datasets\\_base.py\u001b[0m in \u001b[0;36mload_files\u001b[1;34m(container_path, description, categories, load_content, shuffle, encoding, decode_error, random_state)\u001b[0m\n\u001b[0;32m    219\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 221\u001b[1;33m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mencoding\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'C:\\\\Users\\\\zakar\\\\Downloads\\\\ba-text-mining-master\\\\ba-text-mining-master\\\\lab_sessions\\\\lab3\\\\airlinetweets\\\\airlinetweets\\\\positive'"
     ]
    }
   ],
   "source": [
    "def important_features_per_class(vectorizer,classifier,n=80):\n",
    "    class_labels = classifier.classes_\n",
    "    feature_names =vectorizer.get_feature_names()\n",
    "    topn_class1 = sorted(zip(classifier.feature_count_[0], feature_names),reverse=True)[:n]\n",
    "    topn_class2 = sorted(zip(classifier.feature_count_[1], feature_names),reverse=True)[:n]\n",
    "    topn_class3 = sorted(zip(classifier.feature_count_[2], feature_names),reverse=True)[:n]\n",
    "    print(\"Important words in negative documents\")\n",
    "    for coef, feat in topn_class1:\n",
    "        print(class_labels[0], coef, feat)\n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"Important words in neutral documents\")\n",
    "    for coef, feat in topn_class2:\n",
    "        print(class_labels[1], coef, feat) \n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"Important words in positive documents\")\n",
    "    for coef, feat in topn_class3:\n",
    "        print(class_labels[2], coef, feat) \n",
    "        \n",
    "# example of how to call from notebook:\n",
    "important_features_per_class(airline_vec, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional! (will not  be graded)] Question 7\n",
    "Train the model on airline tweets and test it on your own set of tweets\n",
    "+ Train the model with the following settings (airline tweets 80% training and 20% test;  Bag of words representation ('airline_count'), min_df=2)\n",
    "+ Apply the model on your own set of tweets and generate the classification report\n",
    "* [1 point] a. Carry out a quantitative analysis.\n",
    "* [1 point] b. Carry out an error analysis on 10 correctly and 10 incorrectly classified tweets and discuss them\n",
    "* [2 points] c. Compare the results (cf. classification report) with the results obtained by VADER on the same tweets and discuss the differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional! (will not be graded)] Question 8: trying to improve the model\n",
    "* [2 points] a. Think of some ways to improve the scikit-learn Naive Bayes model by playing with the settings or applying linguistic preprocessing (e.g., by filtering on part-of-speech, or removing punctuation). Do not change the classifier but continue using the Naive Bayes classifier. Explain what the effects might be of these other settings \n",
    "+ [1 point] b. Apply the model with at least one new setting (train on the airline tweets using 80% training, 20% test) and generate the scores\n",
    "* [1 point] c. Discuss whether the model achieved what you expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
