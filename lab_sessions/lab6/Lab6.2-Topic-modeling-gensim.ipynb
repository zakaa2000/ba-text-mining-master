{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab6.2: Topic modeling using gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we demonstrate how LDA models can be built and applied using the *gensim* package.\n",
    "\n",
    "Credits:\n",
    "\n",
    "This notebook is an adaptation of a blog from Susan Li's:\n",
    "\n",
    "https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set we’ll use is a list of over one million news headlines published over a period of 15 years and can be downloaded from:\n",
    "\n",
    "https://www.kaggle.com/therohk/million-headlines/data\n",
    "\n",
    "We read the CSV file using the pandas framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zakar\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3444: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
      "\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#### Adapt the path below to point to your local copy of the data set\n",
    "data = pd.read_csv('C:/Users/zakar/Downloads/abcnews-date-text.csv/abcnews-date-text.csv', error_bad_lines=False);\n",
    "data_text = data[['headline_text']]\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1226258\n",
      "                                       headline_text  index\n",
      "0  aba decides against community broadcasting lic...      0\n",
      "1     act fire witnesses must be aware of defamation      1\n",
      "2     a g calls for infrastructure protection summit      2\n",
      "3           air nz staff in aust strike for pay rise      3\n",
      "4      air nz strike to affect australian travellers      4\n"
     ]
    }
   ],
   "source": [
    "print(len(documents))\n",
    "print(documents[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the *gensim* package to build our LDA models from the data.\n",
    "Before building the model, we are going to preprocess the texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing\n",
    "We will perform the following steps:\n",
    "\n",
    "* Tokenization: Split the text into sentences and the sentences into words. Lowercase the words and remove punctuation.\n",
    "* Words that have fewer than 3 characters are removed.\n",
    "* All stopwords are removed.\n",
    "* Words are lemmatized — words in third person are changed to first person and verbs in past and future tenses are changed into present.\n",
    "* Words are stemmed — words are reduced to their root form.\n",
    "\n",
    "In order to apply these processing steps, we first load the gensim and nltk libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\zakar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return lemmatizer.lemmatize(text)\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "           # result.append(token)\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['ratepayers', 'group', 'wants', 'compulsory', 'local', 'govt', 'voting']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['ratepayer', 'group', 'want', 'compulsory', 'local', 'govt', 'voting']\n"
     ]
    }
   ],
   "source": [
    "doc_sample = documents[documents['index'] == 4310].values[0][0]\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now apply the preprocessing to all the headlines and print the first 10 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          [decides, community, broadcasting, licence]\n",
       "1                         [witness, aware, defamation]\n",
       "2           [call, infrastructure, protection, summit]\n",
       "3                          [staff, aust, strike, rise]\n",
       "4              [strike, affect, australian, traveller]\n",
       "5               [ambitious, olsson, win, triple, jump]\n",
       "6          [antic, delighted, record, breaking, barca]\n",
       "7    [aussie, qualifier, stosur, waste, memphis, ma...\n",
       "8             [aust, address, security, council, iraq]\n",
       "9                       [australia, locked, timetable]\n",
       "Name: headline_text, dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs = documents['headline_text'].map(preprocess)\n",
    "### print the first 10 results\n",
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words on the Data set\n",
    "Create a dictionary from ‘processed_docs’ containing the number of times a word appears in the training set.\n",
    "We are going to use the *Dictionary* function to derive a dictionary with counts from the headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 broadcasting\n",
      "1 community\n",
      "2 decides\n",
      "3 licence\n",
      "4 aware\n",
      "5 defamation\n",
      "6 witness\n",
      "7 call\n",
      "8 infrastructure\n",
      "9 protection\n",
      "10 summit\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim filter_extremes\n",
    "Filter out tokens that appear in\n",
    "less than 15 documents (absolute number) or\n",
    "more than 0.5 documents (fraction of total corpus size, not absolute number).\n",
    "after the above two steps, keep only the first 100000 most frequent tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim doc2bow\n",
    "For each document we create a dictionary reporting how many words and how many times those words appear. \n",
    "Gensim provides the *doc2bow* function to create a BoW vector representation for a document.\n",
    "Save this to ‘bow_corpus’, then check our selected document earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(164, 1), (241, 1), (615, 1), (891, 1), (4170, 1), (4171, 1), (4172, 1)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[4310]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preview Bag Of Words for our sample preprocessed document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 164 (\"govt\") appears 1 time.\n",
      "Word 241 (\"group\") appears 1 time.\n",
      "Word 615 (\"local\") appears 1 time.\n",
      "Word 891 (\"want\") appears 1 time.\n",
      "Word 4170 (\"compulsory\") appears 1 time.\n",
      "Word 4171 (\"ratepayer\") appears 1 time.\n",
      "Word 4172 (\"voting\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_4310 = bow_corpus[4310]\n",
    "for i in range(len(bow_doc_4310)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n",
    "                                               dictionary[bow_doc_4310[i][0]], \n",
    "bow_doc_4310[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "Create tf-idf model object using models.TfidfModel on ‘bow_corpus’ and save it to ‘tfidf’, then apply transformation to the entire corpus and call it ‘corpus_tfidf’. Finally we preview TF-IDF scores for our first document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.6151016896202017),\n",
      " (1, 0.33155913313072555),\n",
      " (2, 0.5687633538435918),\n",
      " (3, 0.4338510112798681)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "from pprint import pprint\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running LDA using Bag of Words\n",
    "Train our lda model using gensim.models.LdaMulticore and save it to ‘lda_model’. This takes a while.\n",
    "Look at the documentation of *gensim* for further details:\n",
    "\n",
    "https://radimrehurek.com/gensim/models/ldamulticore.html\n",
    "\n",
    "As parameters, we pass the corpus data as BoW (a list of lists of tuples), the prefixed number of topics, the actual words and the number of passes and workers used for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=3, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each topic, we will explore the words occuring in that topic and its relative weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.021*\"australia\" + 0.014*\"australian\" + 0.013*\"trump\" + 0.013*\"coronavirus\" + 0.011*\"queensland\" + 0.007*\"world\" + 0.007*\"south\" + 0.007*\"home\" + 0.006*\"sydney\" + 0.006*\"coast\"\n",
      "Topic: 1 \n",
      "Words: 0.016*\"coronavirus\" + 0.009*\"victoria\" + 0.009*\"say\" + 0.009*\"government\" + 0.008*\"covid\" + 0.007*\"election\" + 0.006*\"news\" + 0.006*\"health\" + 0.006*\"donald\" + 0.006*\"market\"\n",
      "Topic: 2 \n",
      "Words: 0.018*\"police\" + 0.012*\"woman\" + 0.010*\"death\" + 0.010*\"case\" + 0.009*\"court\" + 0.008*\"china\" + 0.007*\"child\" + 0.007*\"murder\" + 0.007*\"year\" + 0.006*\"family\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you distinguish different topics using the words in each topic and their corresponding weights?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running LDA using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.008*\"coronavirus\" + 0.007*\"australia\" + 0.005*\"queensland\" + 0.004*\"coast\" + 0.004*\"covid\" + 0.004*\"live\" + 0.004*\"south\" + 0.004*\"farmer\" + 0.004*\"victoria\" + 0.003*\"record\"\n",
      "Topic: 1 Word: 0.009*\"police\" + 0.007*\"woman\" + 0.006*\"news\" + 0.006*\"death\" + 0.005*\"market\" + 0.005*\"crash\" + 0.004*\"australian\" + 0.004*\"murder\" + 0.004*\"charged\" + 0.004*\"drum\"\n",
      "Topic: 2 Word: 0.008*\"trump\" + 0.006*\"coronavirus\" + 0.005*\"election\" + 0.005*\"government\" + 0.005*\"donald\" + 0.005*\"rural\" + 0.004*\"say\" + 0.004*\"country\" + 0.004*\"health\" + 0.004*\"change\"\n"
     ]
    }
   ],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=3, id2word=dictionary, passes=2, workers=4)\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, can you distinguish different topics using the words in each topic and their corresponding weights? Do you observe any differences with the BoW version? Do these differences make sense given the information value weighing by the *tfidf* method?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance evaluation by classifying sample document using LDA Bag of Words model\n",
    "We will check where our test document would be classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ratepayer', 'group', 'want', 'compulsory', 'local', 'govt', 'voting']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[4310]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document 4310 is already represented in the correct way. We can directly pass it to our *lda_model* to get the similarity scores for each topic. We represent each topic by printing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.7918939590454102\t \n",
      "Topic: 0.016*\"coronavirus\" + 0.009*\"victoria\" + 0.009*\"say\" + 0.009*\"government\" + 0.008*\"covid\" + 0.007*\"election\" + 0.006*\"news\" + 0.006*\"health\" + 0.006*\"donald\" + 0.006*\"market\"\n",
      "\n",
      "Score: 0.16640225052833557\t \n",
      "Topic: 0.018*\"police\" + 0.012*\"woman\" + 0.010*\"death\" + 0.010*\"case\" + 0.009*\"court\" + 0.008*\"china\" + 0.007*\"child\" + 0.007*\"murder\" + 0.007*\"year\" + 0.006*\"family\"\n",
      "\n",
      "Score: 0.04170382767915726\t \n",
      "Topic: 0.021*\"australia\" + 0.014*\"australian\" + 0.013*\"trump\" + 0.013*\"coronavirus\" + 0.011*\"queensland\" + 0.007*\"world\" + 0.007*\"south\" + 0.007*\"home\" + 0.006*\"sydney\" + 0.006*\"coast\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our test document has the highest probability to be part of the topic that our model assigned, which is the accurate classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing our LDA model\n",
    "\n",
    "Now that we have a trained model let’s visualize the topics for interpretability. \n",
    "To do so, we’ll use a popular visualization package, *pyLDAvis* which is designed to help interactively with:\n",
    "\n",
    "1. Better understanding and interpreting individual topics, and\n",
    "2. Better understanding the relationships between the topics.\n",
    "\n",
    "For (1), you can manually select each topic to view its top most frequent and/or “relevant” terms, using different values of the λ parameter. This can help when you’re trying to assign a human interpretable name or “meaning” to each topic.\n",
    "For (2), exploring the Intertopic Distance Plot can help you learn about how topics relate to each other, including potential higher-level structure between groups of topics.\n",
    "\n",
    "You need to install *pyldavis* through the command line, following the instructions:\n",
    "\n",
    "https://anaconda.org/conda-forge/pyldavis\n",
    "\n",
    "WARNING: running the next cell takes a long time and you need some memory to run it. However, the result is spectacular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zakar\\Anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el1301623268157069122883347356\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el1301623268157069122883347356_data = {\"mdsDat\": {\"x\": [-0.22342662297338828, -0.13333883078004594, 0.35676545375343416], \"y\": [0.2662900455373864, -0.315237755326055, 0.048947709788668414], \"topics\": [1, 2, 3], \"cluster\": [1, 1, 1], \"Freq\": [37.74372857093766, 33.28320972523095, 28.973061703831394]}, \"tinfo\": {\"Term\": [\"australia\", \"police\", \"australian\", \"trump\", \"woman\", \"queensland\", \"death\", \"case\", \"court\", \"victoria\", \"government\", \"child\", \"world\", \"south\", \"murder\", \"home\", \"news\", \"adelaide\", \"health\", \"china\", \"family\", \"market\", \"coast\", \"crash\", \"life\", \"change\", \"charged\", \"state\", \"face\", \"people\", \"government\", \"health\", \"news\", \"market\", \"change\", \"national\", \"plan\", \"farmer\", \"federal\", \"business\", \"water\", \"council\", \"indigenous\", \"labor\", \"community\", \"victorian\", \"rise\", \"price\", \"rural\", \"budget\", \"care\", \"restriction\", \"service\", \"regional\", \"climate\", \"industry\", \"concern\", \"public\", \"rate\", \"funding\", \"victoria\", \"hospital\", \"minister\", \"help\", \"call\", \"worker\", \"report\", \"state\", \"donald\", \"election\", \"say\", \"coronavirus\", \"covid\", \"australia\", \"australian\", \"queensland\", \"world\", \"south\", \"coast\", \"live\", \"final\", \"house\", \"dy\", \"gold\", \"dead\", \"country\", \"border\", \"trump\", \"win\", \"killed\", \"return\", \"street\", \"drum\", \"storm\", \"league\", \"warning\", \"beat\", \"deal\", \"weather\", \"cricket\", \"near\", \"update\", \"east\", \"home\", \"north\", \"crash\", \"open\", \"west\", \"wall\", \"pandemic\", \"chinese\", \"beach\", \"protest\", \"test\", \"scott\", \"melbourne\", \"coronavirus\", \"sydney\", \"island\", \"covid\", \"year\", \"say\", \"police\", \"death\", \"court\", \"case\", \"murder\", \"child\", \"charged\", \"adelaide\", \"life\", \"trial\", \"face\", \"charge\", \"time\", \"interview\", \"accused\", \"royal\", \"guilty\", \"drug\", \"driver\", \"alleged\", \"andrew\", \"abuse\", \"assault\", \"shooting\", \"officer\", \"speaks\", \"john\", \"arrested\", \"baby\", \"jailed\", \"woman\", \"people\", \"morrison\", \"lockdown\", \"victim\", \"game\", \"family\", \"president\", \"china\", \"year\", \"sydney\", \"coronavirus\", \"attack\"], \"Freq\": [38066.0, 29237.0, 26403.0, 23774.0, 19723.0, 20880.0, 15740.0, 15687.0, 15039.0, 18906.0, 18442.0, 11548.0, 12478.0, 12464.0, 10792.0, 12065.0, 13304.0, 9889.0, 12574.0, 14139.0, 10390.0, 11917.0, 10515.0, 10338.0, 8946.0, 11319.0, 8875.0, 11282.0, 7924.0, 7892.0, 18441.913163090187, 12574.02145133315, 13304.18530592378, 11917.102026571029, 11319.32541327614, 9637.346547853524, 8918.125129182992, 8413.812685396279, 8516.187693500844, 8596.468837696022, 8148.563839410551, 7512.248116501715, 8150.464695184003, 7272.741261102829, 7637.998158502579, 8303.061469569986, 6945.696376427348, 6411.388710431833, 6133.629501979045, 6176.13654473911, 6662.388671551718, 9310.944678311838, 6083.2400560142105, 6056.154683847766, 5823.481869111247, 5547.680381715853, 5482.452984866914, 5508.367029389784, 5143.5038147250125, 4915.056528010584, 18878.413026734943, 8723.645858104162, 8198.86774443544, 8316.231858153216, 8790.720647639087, 8063.088027225941, 9719.091608589137, 11088.709955350929, 12153.629529913851, 14111.68886284282, 18650.15869316346, 33556.74646101309, 17334.041344057026, 38065.89557059291, 26403.226932055015, 20880.0974817497, 12478.120586191495, 12464.065414865678, 10514.572778823123, 8608.858011290182, 7472.223497906903, 7851.671979090696, 7793.689790032196, 6526.756715297423, 6409.128980471407, 5978.444741034739, 8755.572844743057, 23771.494034047173, 5694.1380349401525, 5642.913420020036, 5533.956483849035, 5929.264499086024, 5754.778766040918, 4866.772150209191, 4389.603926761562, 4611.633117609098, 4577.350654378606, 4855.158225936854, 4288.110880589924, 3915.096816894528, 3976.2407123282965, 4633.5657680154945, 3661.764718560144, 12062.776858655327, 8670.250917352123, 10308.36969203051, 8454.475707720247, 6273.328612865431, 4754.628649459076, 4966.336971223945, 4790.8671961783475, 5147.166260736498, 6523.655868662604, 7545.196898242905, 5845.879797236124, 9727.213173052858, 23162.284603440563, 11054.269242504863, 6181.84099374146, 8953.361858123815, 7279.960720394753, 7876.476180630926, 29236.38551468024, 15739.842151625393, 15039.165526976256, 15687.032332671972, 10792.395444395323, 11548.064451092221, 8874.9880229665, 9888.740451972646, 8945.772461536855, 7650.663398969345, 7924.256823493613, 6458.314630111238, 7133.937668963613, 5884.706828814527, 5878.3781420163605, 6593.617872276512, 5584.120508585055, 5720.613900540233, 5369.826390614709, 4917.069532921661, 5670.791021780534, 4843.828311791266, 4615.1919561333, 4664.369393157407, 4512.319736111364, 4638.777068709666, 4599.944607665017, 4184.9356450482, 4366.8632850487675, 4046.1500022124037, 19668.8511944068, 7885.695970300705, 6507.072716551067, 5087.09865972485, 5689.718880492518, 4997.650135452372, 10122.4545313424, 4874.238317818017, 12249.577538383666, 10467.020430642076, 8518.951915158726, 8128.026883384451, 5591.365498795136], \"Total\": [38066.0, 29237.0, 26403.0, 23774.0, 19723.0, 20880.0, 15740.0, 15687.0, 15039.0, 18906.0, 18442.0, 11548.0, 12478.0, 12464.0, 10792.0, 12065.0, 13304.0, 9889.0, 12574.0, 14139.0, 10390.0, 11917.0, 10515.0, 10338.0, 8946.0, 11319.0, 8875.0, 11282.0, 7924.0, 7892.0, 18442.508240006693, 12574.613834711628, 13304.828019886758, 11917.70290830703, 11319.92024294417, 9637.953178601814, 8918.710303166923, 8414.401558826425, 8516.784520866477, 8597.09450023252, 8149.167131719218, 7512.83149596293, 8151.102442423613, 7273.326783609867, 7638.614606100481, 8303.742829594856, 6946.3162452696415, 6411.964444504391, 6134.19486989607, 6176.707673031445, 6663.016275170675, 9311.865026349107, 6083.845393866389, 6056.759542304416, 5824.081086843922, 5548.266278202946, 5483.054126964771, 5508.981925469534, 5144.079520035377, 4915.626435357165, 18906.3236810762, 8728.19140307144, 8200.458013484596, 8320.090306012822, 8837.473468852413, 8115.324245883066, 9840.114486696957, 11282.964836911899, 14460.933217381791, 17887.66248223345, 26699.2460706737, 64847.057947838104, 28466.597501056684, 38066.5970586404, 26403.93260546657, 20880.99283751736, 12478.746681471905, 12464.694381246907, 10515.20716229837, 8609.523849369916, 7472.844632156356, 7852.354455485418, 7794.368407744999, 6527.40236826273, 6409.794902753394, 5979.078330996951, 8756.508919987404, 23774.103024780103, 5694.769555408711, 5643.589963395355, 5534.6206859843505, 5929.980053890334, 5755.48755073278, 4867.402859740441, 4390.213711722939, 4612.335713466831, 4578.071876370302, 4855.932494158479, 4288.796081940006, 3915.7313466545884, 3976.8950574511296, 4634.34098878935, 3662.3938961668896, 12065.716790331473, 8673.126642968175, 10338.896843955452, 8467.796889186215, 6279.97245824539, 4755.4902873021565, 4984.523091163586, 4799.766828504075, 5178.33496670601, 6676.851164981412, 8159.707859525512, 6504.629941546282, 14711.55590292777, 64847.057947838104, 19573.599251961965, 7658.25615521872, 28466.597501056684, 17765.378416461725, 26699.2460706737, 29237.00495879313, 15740.479348777517, 15039.782965736758, 15687.794855042093, 10792.991591731361, 11548.70527338779, 8875.58774214209, 9889.449857405962, 8946.479087504666, 7651.293841750646, 7924.926275243099, 6458.919562422799, 7134.637939071496, 5885.305553593203, 5878.98560918226, 6594.302709100072, 5584.717322835195, 5721.231457119253, 5370.45240067293, 4917.67250457813, 5671.515416729902, 4844.448696715334, 4615.794384209542, 4664.989216949908, 4512.936018051718, 4639.421465802064, 4600.610991867212, 4185.547432599392, 4367.509916638798, 4046.7523191019795, 19723.595222110023, 7892.921694014096, 6513.778412454358, 5089.264187158935, 5699.348078241162, 5008.055724854213, 10390.189812301605, 4902.439620110411, 14139.915702608647, 17765.378416461725, 19573.599251961965, 64847.057947838104, 9086.212678852924], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -4.7291, -5.1121, -5.0556, -5.1657, -5.2172, -5.3781, -5.4556, -5.5138, -5.5017, -5.4924, -5.5459, -5.6272, -5.5456, -5.6596, -5.6106, -5.5271, -5.7056, -5.7856, -5.8299, -5.823, -5.7472, -5.4125, -5.8382, -5.8426, -5.8818, -5.9303, -5.9422, -5.9374, -6.006, -6.0514, -4.7057, -5.4777, -5.5397, -5.5255, -5.47, -5.5564, -5.3696, -5.2378, -5.1461, -4.9967, -4.7179, -4.1305, -4.791, -3.8786, -4.2445, -4.4791, -4.994, -4.9951, -5.1652, -5.3652, -5.5068, -5.4572, -5.4646, -5.642, -5.6602, -5.7298, -5.3483, -4.3495, -5.7785, -5.7875, -5.807, -5.738, -5.7679, -5.9355, -6.0387, -5.9894, -5.9968, -5.9379, -6.0621, -6.1531, -6.1376, -5.9846, -6.22, -5.0278, -5.358, -5.185, -5.3832, -5.6816, -5.9588, -5.9153, -5.9512, -5.8795, -5.6425, -5.497, -5.7522, -5.243, -4.3754, -5.1151, -5.6963, -5.3259, -5.5328, -5.4541, -4.0038, -4.6231, -4.6686, -4.6264, -5.0004, -4.9327, -5.196, -5.0879, -5.1881, -5.3445, -5.3093, -5.5139, -5.4144, -5.6069, -5.608, -5.4932, -5.6593, -5.6352, -5.6985, -5.7865, -5.6439, -5.8016, -5.8499, -5.8393, -5.8724, -5.8448, -5.8532, -5.9478, -5.9052, -5.9815, -4.4002, -5.3142, -5.5064, -5.7525, -5.6406, -5.7703, -5.0645, -5.7953, -4.8738, -5.031, -5.237, -5.2839, -5.658], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.9743, 0.9743, 0.9743, 0.9743, 0.9743, 0.9743, 0.9743, 0.9743, 0.9743, 0.9743, 0.9743, 0.9743, 0.9743, 0.9743, 0.9743, 0.9743, 0.9743, 0.9743, 0.9743, 0.9743, 0.9743, 0.9743, 0.9743, 0.9743, 0.9742, 0.9742, 0.9742, 0.9742, 0.9742, 0.9742, 0.9729, 0.9738, 0.9742, 0.9739, 0.969, 0.9679, 0.962, 0.957, 0.8005, 0.7372, 0.6156, 0.3156, 0.4783, 1.1001, 1.1001, 1.1001, 1.1001, 1.1001, 1.1001, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.0999, 1.0999, 1.0999, 1.0998, 1.0972, 1.0985, 1.0991, 1.0999, 1.0965, 1.0983, 1.0941, 1.0769, 1.0218, 0.9933, 0.6864, 0.0706, 0.5288, 0.8859, -0.0566, 0.208, -0.1206, 1.2388, 1.2388, 1.2388, 1.2388, 1.2387, 1.2387, 1.2387, 1.2387, 1.2387, 1.2387, 1.2387, 1.2387, 1.2387, 1.2387, 1.2387, 1.2387, 1.2387, 1.2387, 1.2387, 1.2387, 1.2387, 1.2387, 1.2387, 1.2387, 1.2387, 1.2387, 1.2387, 1.2387, 1.2387, 1.2387, 1.236, 1.2379, 1.2378, 1.2384, 1.2371, 1.2367, 1.2127, 1.233, 1.0953, 0.7098, 0.4069, -0.8379, 0.7533]}, \"token.table\": {\"Topic\": [3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3, 2, 3, 2, 1, 2, 1, 1, 1, 3, 1, 3, 1, 3, 3, 3, 2, 3, 2, 3, 1, 2, 1, 1, 1, 2, 3, 1, 2, 3, 1, 2, 3, 2, 3, 2, 2, 2, 3, 1, 2, 3, 3, 3, 2, 2, 2, 1, 2, 3, 1, 2, 3, 1, 1, 2, 1, 2, 3, 2, 1, 3, 1, 1, 3, 2, 3, 1, 3, 2, 1, 1, 3, 1, 2, 3, 3, 3, 2, 1, 2, 3, 2, 1, 2, 3, 1, 2, 3, 1, 3, 1, 2, 3, 3, 1, 2, 1, 1, 2, 3, 3, 1, 2, 3, 1, 2, 3, 1, 3, 1, 3, 2, 3, 1, 1, 2, 3, 1, 1, 2, 1, 1, 1, 2, 3, 1, 2, 2, 1, 3, 1, 1, 2, 3, 1, 2, 3, 1, 3, 2, 3, 1, 2, 2, 2, 2, 3, 2, 3, 3, 3, 1, 2, 3, 2, 2, 3, 1, 2, 1, 1, 2, 2, 1, 2, 1, 2, 2, 2, 3, 1, 2, 2, 1, 2, 3], \"Freq\": [0.9999073791996933, 0.9998323504686385, 0.9999545113820841, 0.9998632473843054, 0.9999091218674321, 0.9998692088410878, 0.9998278987009778, 0.3846487115731063, 0.6153278816610122, 0.9999843154185944, 0.9999646792968114, 0.9998832477433296, 0.9939488335715094, 0.005986480248827821, 0.9997658672910239, 0.00011420076301383343, 0.9999418809491255, 0.9998854287641724, 0.9998726895194079, 0.9947413172988628, 0.005205107564070947, 0.9998474752081183, 0.9999493329018235, 0.9999187058809231, 0.9998576290641319, 0.9999337799186752, 0.9999389305232843, 0.1336641631923815, 0.8663417984691394, 0.9981734886678219, 0.0016667476329247704, 0.9998143764092907, 0.9999802987905828, 0.9999195395850983, 0.9998077482110587, 0.5174791434176196, 0.3571788872616415, 0.12534107571291866, 0.9998893232247553, 0.999819649294213, 0.9999479403566832, 0.6089241961339623, 0.3145089608853908, 0.07654585343117017, 0.9970115918146997, 0.0029016635384595453, 0.9998132285926068, 0.9998759862420789, 0.9998079680556513, 0.9999695467484252, 0.8404713456107454, 0.15946412069922486, 6.915183031189282e-05, 0.9999157611614111, 0.9999595441783841, 0.9999152894123249, 0.9999527341118963, 0.9998924484427244, 0.7889236513723609, 0.21109521737401038, 0.9998831187558183, 0.025697316875181793, 9.624463249131758e-05, 0.9741881700771166, 0.9999522771971818, 0.9999078853216781, 0.9998869731410284, 0.9998725624566059, 0.001996782893283622, 0.9979920900631541, 0.9999383570615032, 0.9999724419260071, 0.9998715561068308, 0.9999511846073608, 0.999508382017216, 0.00048076401251429343, 0.9997748339051311, 0.00024863835710149993, 0.9995197856144671, 0.00045828509198279097, 0.9999548599738554, 0.9998647492861977, 0.9999520069532366, 0.9999480819491154, 0.16661756594945829, 0.8072333798585825, 0.026246183977931912, 0.9998140931189615, 0.9998671933209976, 0.99989546310076, 0.9999550709572678, 0.9999513208839087, 0.9999464496032484, 0.999939154664174, 0.00019649205921028177, 0.00019649205921028177, 0.9995551052027034, 0.9999410198162818, 0.6611809154777581, 0.33878129770136184, 0.9998222034083707, 0.00012194440827032207, 0.0007676036369981499, 0.00030704145479925997, 0.9989593731893923, 0.9999081263315241, 0.9999011015529801, 0.9997749356122806, 0.9999377654573572, 0.0002305973476844732, 0.9996395022121913, 0.0001152986738422366, 0.9997925922175777, 0.0015352281319597578, 0.9983706636605993, 0.00011809447168921213, 0.003209935977298274, 0.9962838789539518, 0.00040124199716228427, 0.0008868705748479327, 0.9991230504643996, 0.9999203580851067, 0.9999656271634338, 0.005711442092043429, 0.9941988841649884, 0.9998495867354321, 0.0004493135949673894, 0.9771072978557495, 0.022615450946691932, 0.9998217591774999, 4.789044313081115e-05, 0.9999524525713368, 0.999984541445157, 0.9998745959288774, 0.9876917604098312, 0.002032496677456181, 0.010264108221153714, 0.9999071049304668, 0.00010738987272371031, 0.9998878539253968, 0.999954472952501, 0.9999540953587626, 0.9999682321966936, 0.6985215968508209, 0.2949896030454191, 0.006479583713415122, 0.00015373664743213353, 0.8987444408882526, 0.10115871401034386, 0.9998610428418774, 0.9997879487167272, 0.999944292156256, 0.9999091555261426, 0.9828090542055623, 0.01719406227034711, 0.9999172331216359, 0.9998347289735501, 0.5647402839767448, 0.43522910070543597, 0.9246654573781202, 0.07524779202520422, 0.9999105856419144, 0.9999615958089281, 8.412515071190572e-05, 0.9998694787863556, 4.206257535595286e-05, 0.9999264212991286, 0.0015791279768224704, 0.998359798679984, 0.998501893781468, 0.001428093608014601, 0.9999105427985789, 0.00021028325989228574, 0.9998969007878188, 0.9999272140000887, 0.9999794909447166, 0.9998143810232997, 0.0009554182028493141, 0.998889731078958, 0.9998648662775159, 0.0027378375692615285, 0.9972319842556482, 0.9935524146296913, 0.006407630604085818, 0.9999401637447282, 0.0010132066752555561, 0.409785810881136, 0.5891796816611059], \"Term\": [\"abuse\", \"accused\", \"adelaide\", \"alleged\", \"andrew\", \"arrested\", \"assault\", \"attack\", \"attack\", \"australia\", \"australian\", \"baby\", \"beach\", \"beach\", \"beat\", \"border\", \"border\", \"budget\", \"business\", \"call\", \"call\", \"care\", \"case\", \"change\", \"charge\", \"charged\", \"child\", \"china\", \"china\", \"chinese\", \"chinese\", \"climate\", \"coast\", \"community\", \"concern\", \"coronavirus\", \"coronavirus\", \"coronavirus\", \"council\", \"country\", \"court\", \"covid\", \"covid\", \"covid\", \"crash\", \"crash\", \"cricket\", \"dead\", \"deal\", \"death\", \"donald\", \"donald\", \"donald\", \"driver\", \"drug\", \"drum\", \"dy\", \"east\", \"election\", \"election\", \"face\", \"family\", \"family\", \"family\", \"farmer\", \"federal\", \"final\", \"funding\", \"game\", \"game\", \"gold\", \"government\", \"guilty\", \"health\", \"help\", \"help\", \"home\", \"home\", \"hospital\", \"hospital\", \"house\", \"indigenous\", \"industry\", \"interview\", \"island\", \"island\", \"island\", \"jailed\", \"john\", \"killed\", \"labor\", \"league\", \"life\", \"live\", \"lockdown\", \"lockdown\", \"lockdown\", \"market\", \"melbourne\", \"melbourne\", \"minister\", \"minister\", \"morrison\", \"morrison\", \"morrison\", \"murder\", \"national\", \"near\", \"news\", \"north\", \"north\", \"north\", \"officer\", \"open\", \"open\", \"open\", \"pandemic\", \"pandemic\", \"pandemic\", \"people\", \"people\", \"plan\", \"police\", \"president\", \"president\", \"price\", \"protest\", \"protest\", \"protest\", \"public\", \"queensland\", \"queensland\", \"rate\", \"regional\", \"report\", \"report\", \"report\", \"restriction\", \"restriction\", \"return\", \"rise\", \"royal\", \"rural\", \"say\", \"say\", \"say\", \"scott\", \"scott\", \"scott\", \"service\", \"shooting\", \"south\", \"speaks\", \"state\", \"state\", \"storm\", \"street\", \"sydney\", \"sydney\", \"test\", \"test\", \"time\", \"trial\", \"trump\", \"trump\", \"trump\", \"update\", \"victim\", \"victim\", \"victoria\", \"victoria\", \"victorian\", \"wall\", \"wall\", \"warning\", \"water\", \"weather\", \"west\", \"west\", \"win\", \"woman\", \"woman\", \"worker\", \"worker\", \"world\", \"year\", \"year\", \"year\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [2, 1, 3]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el1301623268157069122883347356\", ldavis_el1301623268157069122883347356_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el1301623268157069122883347356\", ldavis_el1301623268157069122883347356_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el1301623268157069122883347356\", ldavis_el1301623268157069122883347356_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "vis = pyLDAvis.gensim_models.prepare(topic_model=lda_model, corpus=bow_corpus, dictionary=dictionary)\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some other useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18728 : coronavirus : 0.016075281\n",
      "1619 : victoria : 0.00904366\n",
      "413 : say : 0.008934315\n",
      "2722 : government : 0.008834556\n",
      "18729 : covid : 0.008303832\n",
      "1019 : election : 0.0067601716\n",
      "1363 : news : 0.006373339\n",
      "259 : health : 0.0060235555\n",
      "1685 : donald : 0.005822168\n",
      "1188 : market : 0.0057088602\n",
      "177 : change : 0.0054224967\n",
      "59 : state : 0.005312021\n",
      "64 : report : 0.0046559083\n",
      "190 : national : 0.0046167485\n",
      "363 : restriction : 0.0044603865\n",
      "47 : plan : 0.004272207\n",
      "7 : call : 0.0042111743\n",
      "369 : hospital : 0.0041790423\n",
      "81 : business : 0.0041181184\n",
      "466 : federal : 0.00407966\n"
     ]
    }
   ],
   "source": [
    "#get the top 20 words and their weights for a specific topic\n",
    "topic_id=1\n",
    "top_terms=20\n",
    "for wordid, score in lda_model.get_topic_terms(topic_id, top_terms):\n",
    "    print(wordid, \":\", dictionary[wordid], \":\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Utility function to get the id for a word\n",
    "\n",
    "def get_id_for_word(dictionary, word):\n",
    "    for k, v in dictionary.iteritems():\n",
    "        if (v==word):\n",
    "            return k\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_terms=20\n",
    "index=get_id_for_word(dictionary,'market')\n",
    "for topic_id, score in lda_model.get_term_topics(index):\n",
    "    print(\"Topic:\", topic_id)\n",
    "    for wordid, score in lda_model.get_topic_terms(topic_id, top_terms):\n",
    "        print(wordid, \":\", dictionary[wordid], \":\", score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading your model for re-use\n",
    "\n",
    "Building a model takes time.Once you have a stable model, you can save it to disk and reload it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LdaModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13016/1149355342.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Load a potentially pretrained model from disk.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mloaded_lda\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLdaModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'LdaModel' is not defined"
     ]
    }
   ],
   "source": [
    "# Save model to disk.\n",
    "temp_file = \"./model\"\n",
    "lda_model.save(temp_file)\n",
    "\n",
    "# Load a potentially pretrained model from disk.\n",
    "loaded_lda = LdaModel.load(temp_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing model on unseen document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_document = 'How a Pentagon deal became an identity crisis for Google'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compare any new text against the topic model, we first need to process it in the same way as we processed the input texts for the model.\n",
    "We apply the same preprocessing function and next apply the *doc2bow* function to represent it using the same vector representation as we used for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(888, 1), (1088, 1), (1920, 1), (5640, 1), (12290, 1)]\n"
     ]
    }
   ],
   "source": [
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "print(bow_vector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now pass this representation of the unseen document into the model to compare it against all the topics.\n",
    "The next function returns in index to the topics and a similarity score for the new document. We print the scores and the topics with the top 5 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.3501623868942261\t Topic_id 5\t Topic: 0.028*\"australian\" + 0.026*\"election\" + 0.019*\"china\" + 0.019*\"donald\" + 0.019*\"south\"\n",
      "Score: 0.35012122988700867\t Topic_id 1\t Topic: 0.047*\"trump\" + 0.024*\"market\" + 0.024*\"home\" + 0.018*\"sydney\" + 0.016*\"island\"\n",
      "Score: 0.18297365307807922\t Topic_id 2\t Topic: 0.027*\"world\" + 0.017*\"australian\" + 0.016*\"tasmania\" + 0.015*\"farmer\" + 0.014*\"final\"\n",
      "Score: 0.01668226346373558\t Topic_id 0\t Topic: 0.018*\"open\" + 0.015*\"tasmanian\" + 0.015*\"case\" + 0.013*\"health\" + 0.011*\"liberal\"\n",
      "Score: 0.016678284853696823\t Topic_id 7\t Topic: 0.018*\"government\" + 0.015*\"change\" + 0.014*\"federal\" + 0.013*\"rural\" + 0.013*\"country\"\n",
      "Score: 0.016676433384418488\t Topic_id 3\t Topic: 0.022*\"crash\" + 0.022*\"adelaide\" + 0.016*\"dy\" + 0.016*\"people\" + 0.016*\"royal\"\n",
      "Score: 0.016676433384418488\t Topic_id 4\t Topic: 0.018*\"test\" + 0.013*\"plan\" + 0.012*\"australia\" + 0.012*\"work\" + 0.011*\"park\"\n",
      "Score: 0.016676433384418488\t Topic_id 6\t Topic: 0.021*\"water\" + 0.019*\"help\" + 0.016*\"victoria\" + 0.015*\"missing\" + 0.013*\"melbourne\"\n",
      "Score: 0.016676433384418488\t Topic_id 8\t Topic: 0.015*\"business\" + 0.014*\"interview\" + 0.014*\"live\" + 0.014*\"street\" + 0.014*\"guilty\"\n",
      "Score: 0.016676433384418488\t Topic_id 9\t Topic: 0.049*\"police\" + 0.026*\"court\" + 0.024*\"woman\" + 0.024*\"queensland\" + 0.020*\"murder\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic_id {}\\t Topic: {}\".format(score, index, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This text matches best with topic 5 although the score is not very high!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the model with a new document\n",
    "\n",
    "We can also use the unseen documents to extend our model and update the topics. This is useful when processing texts in a stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piek/opt/anaconda3/lib/python3.7/site-packages/gensim/models/ldamodel.py:824: RuntimeWarning: overflow encountered in exp2\n",
      "  perwordbound, np.exp2(-perwordbound), len(chunk), corpus_words\n"
     ]
    }
   ],
   "source": [
    "# Update the model by incrementally training on the new corpus.\n",
    "\n",
    "other_texts = [['computer', 'time', 'graph'],['survey', 'response', 'eps'],['human', 'system', 'computer']]\n",
    "other_corpus = [dictionary.doc2bow(text) for text in other_texts]\n",
    "\n",
    "# Update the model by incrementally training on the new corpus.\n",
    "lda_model.update(other_corpus)  # update the LDA model with additional documents\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
