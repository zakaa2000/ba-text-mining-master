{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1 - Introduction to the [Natural Language Toolkit](https://www.nltk.org/) (NLTK)\n",
    "\n",
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL\n",
    "\n",
    "In this notebook, we will use the [Natural Language Toolkit](https://www.nltk.org/) to perform various Natural Language Processing tasks including sentence splitting, stop word recognition, and Named Entity Recognition (NER). The Natural Language Toolkit is a python package that provides easy access to [popular corpora and lexical resources](https://www.nltk.org/book/ch02.html#tab-corpora). Also, it contains a wide range of text processing modules (hence the name **toolkit**). NLTK is perfect for getting started with Natural Language Processing since it allows you to study each NLP task separately, which means that you can analyze the input, the algorithm, and the output.  NLTK is an open source and community-driven project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Main goal of this notebook**: The most important goal of this notebook is to show you how to perform various NLP tasks using NLTK. It is important that you can use the code snippets from this notebook on other language data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**At the end of this notebook, you will be able to perform the following NLP tasks using NLTK**:\n",
    "* **Sentence splitting**: *nltk.tokenize.sent_tokenize*\n",
    "* **Tokenization**: *nltk.word_tokenize*\n",
    "* **Part-of-speech (POS) tagging**: *nltk.pos_tag* \n",
    "* **Stop words recognition** \n",
    "* **Stemming and lemmatization**\n",
    "     * *nltk.stem.porter.PorterStemmer*\n",
    "     * *nltk.stem.snowball.SnowballStemmer*\n",
    "     * *nltk.stem.wordnet.WordNetLemmatizer*\n",
    "* **Constituency/dependency parsing** *nltk.RegexpParser*\n",
    "* **Named Entity Recognition (NER)** *nltk.chunk.ne_chunk*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you want to learn more about these topics, you might find the following links useful (information from these blogs is used in this notebook):**\n",
    "* [NLTK book](https://www.nltk.org/book/)\n",
    "* [official NLTK website](https://www.nltk.org/)\n",
    "* [an introduction to NLTK](https://www.pythonforengineers.com/introduction-to-nltk-natural-language-processing-with-python/)\n",
    "* [another introduction to NLTK](https://nlpforhackers.io/introduction-nltk/)\n",
    "* [yet another introduction to NLTK](https://textminingonline.com/dive-into-nltk-part-i-getting-started-with-nltk)\n",
    "* [introduction to tokenization from Stanford](https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html)\n",
    "* [introduction to part of speech tagging](http://aritter.github.io/courses/5525_slides/pos1.pdf)\n",
    "* [introduction to stemming and lemmatization](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)\n",
    "* [comparison stemming and lemmatization](https://blog.bitext.com/what-is-the-difference-between-stemming-and-lemmatization/)\n",
    "* [introduction to Named Entity Recognition](https://www.codementor.io/bofinbabu/introduction-to-named-entity-recognition-ner-k584v86r6)\n",
    "* [introduction to Named Entity Recognition using NLTK](https://nlpforhackers.io/named-entity-extraction/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started ([NLTK Chapter 1, Section 1.2](https://www.nltk.org/book/ch01.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please try to import the NLTK module by running the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint ### needed to print python data more elegantly\n",
    "import nltk  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get an error, such as *ModuleNotFoundError: No module named 'nltk'*, please install the module using for example `conda install -c anaconda nltk ` and try again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading data sets\n",
    "The first time you import NLTK on your local machine, you will need to download the data sets that we will use in this course. When you run the cell below, you will get a pop-up window to select which data sets to download. The minimal data set you need is `book`. Take your time to check out the different TABs and get an idea of what is there. Make sure you have sufficient disk space to store what you want.\n",
    "\n",
    "If you have already run the download command, you can skip the next step as the data are in your local drive. If you need another dataset, rerun it and take your pick.\n",
    "\n",
    "**Tip:** comment out *nltk.download()* after you've used it, such that you can use *Restart kernel and run all cells*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#nltk.download() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: When you run `nltk.download()` a star will appear to the left of the code cell. A program should open, that looks like [this](https://i.stack.imgur.com/hw89E.jpg). If the program does not show, you can simply replace `nltk.download()` by `nltk.download('book')`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please run the following cell to check that you can import the Brown corpus (which is part of the *book* data set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have everything installed, we can get started with some examples of text processing using NLTK!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence splitting ([NLTK Chapter 3, Section 3.8](https://www.nltk.org/book/ch03.html))\n",
    "Consider the following input that is given to a computer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Another ex-Golden Stater, Paul Stankowski from Oxnard, is contending for a berth on the U.S. Ryder Cup team after winning his first PGA Tour event last year and staying within three strokes of the lead through three rounds of last month's U.S. Open. H.J. Heinz Company said it completed the sale of its Ore-Ida frozen-food business catering to the service industry to McCain Foods Ltd. for about $500 million. It's the first group action of its kind in Britain.\n"
     ]
    }
   ],
   "source": [
    "a_text = '''Another ex-Golden Stater, Paul Stankowski from Oxnard, is contending for a berth on the U.S. Ryder Cup team after winning his first PGA Tour event last year and staying within three strokes of the lead through three rounds of last month's U.S. Open. H.J. Heinz Company said it completed the sale of its Ore-Ida frozen-food business catering to the service industry to McCain Foods Ltd. for about $500 million. It's the first group action of its kind in Britain.'''\n",
    "print(a_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before the computer can apply most kinds of NLP tasks, it has to know what the separate sentences are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try splitting the text using a **dot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTENCE: 1 Another ex-Golden Stater, Paul Stankowski from Oxnard, is contending for a berth on the U\n",
      "SENTENCE: 2 S\n",
      "SENTENCE: 3  Ryder Cup team after winning his first PGA Tour event last year and staying within three strokes of the lead through three rounds of last month's U\n",
      "SENTENCE: 4 S\n",
      "SENTENCE: 5  Open\n",
      "SENTENCE: 6  H\n",
      "SENTENCE: 7 J\n",
      "SENTENCE: 8  Heinz Company said it completed the sale of its Ore-Ida frozen-food business catering to the service industry to McCain Foods Ltd\n",
      "SENTENCE: 9  for about $500 million\n",
      "SENTENCE: 10  It's the first group action of its kind in Britain\n",
      "SENTENCE: 11 \n"
     ]
    }
   ],
   "source": [
    "dot_splitted_text = a_text.split('.')\n",
    "for index, sentence in enumerate(dot_splitted_text, 1):\n",
    "    print(f'SENTENCE: {index} {sentence}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This clearly did not work. Many abbreviations such us **U.S.** have dots in them. However, sentences normally start with a capital letter. What would happen if we split a text using a dot followed by a space followed by a capital letter? This should work, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this we need to be able to define a pattern. We are going to use the Regular Expressions package *re* to define a pattern. This is explained in [NLTK Chapter 3, Section 3.4](https://www.nltk.org/book/ch03.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTENCE: 1 Another ex-Golden Stater, Paul Stankowski from Oxnard, is contending for a berth on the U.S\n",
      "SENTENCE: 2 yder Cup team after winning his first PGA Tour event last year and staying within three strokes of the lead through three rounds of last month's U.S\n",
      "SENTENCE: 3 pen\n",
      "SENTENCE: 4 .J\n",
      "SENTENCE: 5 einz Company said it completed the sale of its Ore-Ida frozen-food business catering to the service industry to McCain Foods Ltd. for about $500 million\n",
      "SENTENCE: 6 t's the first group action of its kind in Britain.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "splitted_using_dot_space_capital = re.split('\\. [A-Z]', a_text)\n",
    "for index, sentence in enumerate(splitted_using_dot_space_capital, 1):\n",
    "    print(f'SENTENCE: {index} {sentence}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately no. In our text, we have the following sequences:\n",
    "* **U.S. Ryder**\n",
    "* **H.J. Heinz Company**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion, it is actually not that easy. Luckily, NLTK contains models that are more complex than what we've just seen. Let's see how it performs on our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\zakar/nltk_data'\n    - 'C:\\\\Users\\\\zakar\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\zakar\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\zakar\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\zakar\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8560/1033617327.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnltk_sentence_splitted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnltk_sentence_splitted\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'SENTENCE: {index} {sentence}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \"\"\"\n\u001b[1;32m--> 106\u001b[1;33m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"tokenizers/punkt/{language}.pickle\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    748\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    749\u001b[0m     \u001b[1;31m# Load the resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 750\u001b[1;33m     \u001b[0mopened_resource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    751\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    752\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"raw\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    874\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    875\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"nltk\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 876\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    877\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"file\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    878\u001b[0m         \u001b[1;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"*\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\zakar/nltk_data'\n    - 'C:\\\\Users\\\\zakar\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\zakar\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\zakar\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\zakar\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "nltk_sentence_splitted = sent_tokenize(a_text)\n",
    "for index, sentence in enumerate(nltk_sentence_splitted, 1):\n",
    "    print(f'SENTENCE: {index} {sentence}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the model is not perfect. It correctly determines that *U.S. Ryder Cup* is not the end of the sentence. However, it states that **H.J.** is the end of a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization ([NLTK Chapter 5, Section 1](https://www.nltk.org/book/ch05.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the first steps of Natural Language Processing is tokenization. It is generally defined as chopping a text into pieces, which are called tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most naive way to apply tokenization is to split a text using spaces. Let's try this. Please run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sentence = \"I'll refuse to permit you to obtain the refuse permit.\"\n",
    "tokenized_using_spaces = example_sentence.split(' ')\n",
    "print(tokenized_using_spaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about the above line, is it actually the same as tokenizing? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, yes and no. Tokenizing using spaces works for most tokens. However, it does not work for expressions such as **I'll**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a real tokenizer...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_using_tokenizer = nltk.word_tokenize(example_sentence)\n",
    "print(tokenized_using_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that **I'll** is now correctly tokenized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of speech tagging ([Chapter 5, Section 1 Using a Tagger](https://www.nltk.org/book/ch05.html))\n",
    "Now that we've established the tokens in a text, a useful next step is to determine the part of speech of each token.\n",
    "The part of speech is the syntactic category of a token. \n",
    "\n",
    "| the | red   | clown  | behaved  | weirdly  |\n",
    "|---|---|---|---|---|\n",
    "| determiner | adjective | noun | verb | adverb |\n",
    "\n",
    "We can replace tokens with another token with the same part of speech, and the sentence would still be grammatical. For example:\n",
    "* The **blue** clown behaved weirdly.\n",
    "* The red **cow** behaved weirdly.\n",
    "* The red clown **walked** weirdly.\n",
    "\n",
    "NLTK also provides a method to automatically tag each token in a text with a part of speech tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.pos_tag(['I', \"'ll\", 'refuse', 'to', 'permit', 'you', 'to', 'obtain', 'the', 'refuse', 'permit', '.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that each token has now been tagged with a part of speech tag. You might be surprised to see **VB** instead of **verb**. The main reason is that there is not one group of part of speech labels, there are [many](https://www.sketchengine.eu/tagsets/english-part-of-speech-tagset/)! The most popular tagset in NLP is the [Penn Treebank POS tagset](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html). This is also the default one used in NLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks**: \n",
    "* Make sure you know what each tag means. \n",
    "* Try some other sentences to get an idea of how the tagger works and where it fails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Stop words\n",
    "An important step in preprocessing data is to remove words that are likely irrelevant to the type of NLP task that you want to perform.\n",
    "It's not uncommon to remove the most commonly used words, so-called *stop words*. NLTK actually keeps lists of stop words for many languages.\n",
    "We show how to remove stop words for English text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stopwords = stopwords.words('english')\n",
    "set_english_stopwords = set(english_stopwords) # sets are faster to check if an element is in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(english_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_sentence = ['the', 'rain', 'on', 'the', 'roof', 'was', 'soothing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "without_stopwords = []\n",
    "\n",
    "for token in a_sentence:\n",
    "    if token not in set_english_stopwords:\n",
    "        without_stopwords.append(token)\n",
    "\n",
    "print(without_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes! We've managed to remove the stopwords!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "* What are stopwords and why would you want to remove these from a text?\n",
    "* How would you make a stop word list automatically?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning up text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text you want to analyze can sometimes be messy. Punctuation can be attached to words that are at the end of a sentence, e.g., **data.** in the example sentence below, or there are just strange characters attached to words, e.g., an underscore in **works** in the example sentence below. It is important to clean your text before analyzing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messy_sentence = \"The point of this example is to _learn how basic text cleaning works_ on *very simple* data.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, we will remove all occurrences from the following characters from our example sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first tokenize our example sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_messy_sentence = nltk.word_tokenize(messy_sentence)\n",
    "print(tokenized_messy_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we clean the tokens of these unwanted characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = {ord(char): '' for char in string.punctuation} # in case you're interested, this is called a 'dict comprehension'\n",
    "\n",
    "cleaned_messy_sentence = []\n",
    "for messy_word in tokenized_messy_sentence:\n",
    "    \n",
    "    cleaned_word = messy_word.translate(table) # the translate method allows us to remove all unwanted charachters\n",
    "    print()\n",
    "    print('OLD', messy_word)\n",
    "    print('NEW', cleaned_word)\n",
    "    cleaned_messy_sentence.append(cleaned_word)\n",
    "\n",
    "print(cleaned_messy_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result of cleaning out the asterisks, which were tokens by themselves, we've now ended up with some empty strings. If we want to remove them, we can add each token from `cleaned_messy_sentence` to a new list `cleaned_sentence`, unless it equals an empty string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_sentence = [token for token in cleaned_messy_sentence if token != ''] # this is known as a 'list comprehension'\n",
    "print(cleaned_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and lemmatizing ([NLTK book Chapter 3, Section 3.6](https://www.nltk.org/book/ch03.html))\n",
    "NLTK has various modules for stripping inflection of words (stemming) or finding the lemma (the form you can find in a dictionary). Below is a script to stem and lemmatize the words in a text example after tokenizing the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw=\"SHUT UP! Enough already, Ballstein! Who cares about Derek Zoolander anyway? The man has only one look, for Christ's sake! Blue Steel? Ferrari? Le Tigra?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Stemming and Lemmatizing\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "wordnet = WordNetLemmatizer()\n",
    "tokens = nltk.word_tokenize(raw)\n",
    "\n",
    "porterlemmas = []\n",
    "wordnetlemmas = []\n",
    "snowballlemmas = []\n",
    "\n",
    "for word in tokens:\n",
    "    porterlemmas.append(porter.stem(word))\n",
    "    snowballlemmas.append(snowball.stem(word))\n",
    "    wordnetlemmas.append(wordnet.lemmatize(word))\n",
    "\n",
    "print('Porter')\n",
    "print(porterlemmas)\n",
    "print('Snowball')\n",
    "print(snowballlemmas)\n",
    "print('Wordnet')\n",
    "print(wordnetlemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question:\n",
    "* What difference do you notice between the three lists?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition (NER) ([NLTK Chapter 7, Section 5](https://www.nltk.org/book/ch07.html))\n",
    "In Named Entity Recognition, the goal is to determine which noun phrases refer to named entities.\n",
    "Named entities can be persons, locations, organizations, etc. (see [NLTK Chapter 7, Section 5](https://www.nltk.org/book/ch07.html) for more information on the task)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.chunk import ne_chunk\n",
    "\n",
    "text = '''In August, Samsung lost a US patent case to Apple and was ordered to pay its rival $1.05bn (£0.66bn) in damages for copying features of the iPad and iPhone in its Galaxy range of devices. Samsung, which is the world's top mobile phone maker, is appealing the ruling. A similar case in the UK found in Samsung's favour and ordered Apple to publish an apology making clear that the South Korean firm had not copied its iPad when designing its own devices.'''\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "for sentence in sentences:\n",
    "    \n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    tokens_pos_tagged = nltk.pos_tag(tokens)\n",
    "    tokens_pos_tagged_and_named_entities = ne_chunk(tokens_pos_tagged)\n",
    "    print()\n",
    "    print('ORIGINAL SENTENCE', sentence)\n",
    "    print('NAMED ENTITY RECOGNITION OUTPUT', tokens_pos_tagged_and_named_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please observe that for textual mentions such as **August** and **Samsung**, a named entity label is assigned.\n",
    "The most frequently used named entity labels are:\n",
    "* ORGANIZATION (e.g., Georgia-Pacific Corp.)\n",
    "* PERSON (e.g., Eddy Bonte, President Obama)\n",
    "* LOCATION (e.g., Murray River, Mount Everest)\n",
    "* DATE (e.g., June, 2008-06-29)\n",
    "* TIME (e.g., two fifty a m, 1:30 p.m.)\n",
    "* MONEY (e.g., 175 million Canadian Dollars, GBP 10.40)\n",
    "* PERCENT (e.g., twenty pct, 18.75 %)\n",
    "* FACILITY (e.g., Washington Monument, Stonehenge)\n",
    "* GPE (=Geo-Political Entity, e.g., South East Asia, Midlothian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please try to understand the output from NLTK regarding named entity recognition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "* What do you think of the performance of the NER module in the NLTK?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constituency/dependency parsing ([NLTK Book Chapter 7, Section 2.1](https://www.nltk.org/book/ch07.html))\n",
    "Please consider the following sentence.\n",
    "- **the cat saw the dog.**\n",
    "\n",
    "As a speaker of English, you immediately start to parse the sentence. You determine that **the cat** is the subject, **saw** is the main verb, and **the dog** is the direct object. With **constituency/dependency parsing**, we attempt to teach computers to parse sentence just like humans do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a module called **RegexpParser**, which is part of NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"),\n",
    "            (\"dog\", \"NN\"), (\"saw\", \"VBD\"), (\"the\", \"DT\"), (\"cat\", \"NN\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create a very simple grammar, that we can extend later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = 'NP: {<DT><NN>}'\n",
    "constituent_parser = nltk.RegexpParser(grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our grammar now only contains one rule, which states that a noun phrase (NP) consists of a determiner (DT) followed by a singular noun (NN). \n",
    "The tags come from the [Penn Treebank](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html).\n",
    "\n",
    "Let's try to parse our example sentence 'the cat saw the dog'. We can inspect the parse result as a visualized tree structure, as well as by printing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constituent_structure = constituent_parser.parse(sentence)\n",
    "print(constituent_structure) # print the sentence structure\n",
    "constituent_structure.draw() # visualize the parse tree structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that **the cat** has now been identified as a noun phrase (NP). However, **the little dog** has not been identified, because we did not include that it was possible to have adjectives between the determiner and the noun. Let's fix that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = 'NP: {<DT><JJ>*<NN>}'\n",
    "constituent_parser = nltk.RegexpParser(grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now changed the rule for an NP. A noun phrase is now defined as:\n",
    "* determiner (DT) followed by one adjective (JJ) followed by a singular noun (NN)\n",
    "\n",
    "The star is needed to indicate that the adjective is optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constituent_structure = constituent_parser.parse(sentence)\n",
    "print(constituent_structure) # print the sentence structure\n",
    "constituent_structure.draw() # visualize the parse tree structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can continue to extend the grammar. Try to understand the following grammar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constituent_parser = nltk.RegexpParser('''\n",
    "NP: {<DT>? <JJ>* <NN>*} # NP\n",
    "P: {<IN>}           # Preposition\n",
    "V: {<V.*>}          # Verb\n",
    "PP: {<P> <NP>}      # PP -> P NP\n",
    "VP: {<V> <NP|PP>*}  # VP -> V (NP|PP)*''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = ['In', 'the', 'house', 'the', 'yellow', 'cat', 'saw', 'the', 'dog']\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "print(tagged)\n",
    "constituent_structure = constituent_parser.parse(tagged)\n",
    "print(constituent_structure)\n",
    "constituent_structure.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save tree structure to file\n",
    "There are at least two ways of saving the tree structure to a file:\n",
    "1. you can drag the image to your Desktop or File Explorer/Finder.\n",
    "2. you can use the code snippet below (might need Ghostscript installation):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please convert the .ps file to PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get the following error:\n",
    "```\n",
    "===========================================================================\n",
    "NLTK was unable to find the gs file!\n",
    "Use software specific configuration paramaters or set the PATH environment variable.\n",
    "```\n",
    "\n",
    "**Solution**:\n",
    "* Download ghost script  and add it to path\n",
    "* How to download: https://wiki.scribus.net/canvas/Installation_and_Configuration_of_Ghostscript\n",
    "* On Windows it is not added to Path automatically; add C:\\Program Files\\gs\\gs9.19\\bin\n",
    "* Kernel needs to be restarted to reload PATH. Probably, even Anaconda needs to be restarted to know the new environment variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
