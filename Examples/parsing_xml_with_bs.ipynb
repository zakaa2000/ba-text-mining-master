{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing XML/HTML using BeautifulSoup\n",
    "\n",
    "This notebook illustrates how to work with XML/HTML data using a Python package called BeautifulSoup. Both XML and HTMl are markup languages. Some datasets are provided in XML (Extensible Markup Language), and when you use a web scraper to gather data from websites, chances are your data is in HTML (HyperText Markup Language), since that is the standard markup language for web browser documents (i.e. websites). This notebook is a primer on how to get information from XML/HTML into a format that is processable in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The basic structure of XML/HTML\n",
    "\n",
    "An XML/HTML document consists of elements demarcated by tags, which are organized in a tree structure. Elements can be nested into each other, and multiple elements can be the 'children' of one parent element.\n",
    "\n",
    "### Tags\n",
    "A tag begins with < and ends with >. There are three kinds of tags:\n",
    "- start-tag, e.g. \\<section>\n",
    "- end-tag, e.g. \\</section>\n",
    "- empty-element tag, e.g. \\<line-break />\n",
    "\n",
    "Start-tags and end-tags always appear in pairs (see 'Elements').\n",
    "\n",
    "### Content\n",
    "Any text that is not inside a tag. When processing scraped data for text mining, this is usually where the data you are looking for is.\n",
    "\n",
    "### Elements\n",
    "An element is component of a document that either begins with a start-tag and ends with a matching end-tag, or consists only of an empty-element tag. The characters between the start-tag and end-tag, if any, are the element's content, and may contain markup, including other elements, which are called child elements. An example is <greeting>Hello, world!</greeting>. Another is <line-break />.\n",
    "\n",
    "### Attributes\n",
    "Attributes appear within a start-tag or empty-element tag, and consist of a name–value pair. An example is <img src=\"madonna.jpg\" alt=\"Madonna\" />, where the names of the attributes are \"src\" and \"alt\", and their values are \"madonna.jpg\" and \"Madonna\", respectively. Attributes usually do not contain running text, but they may still be useful in the context of text mining as they may provide useful meta-information or even labels.\n",
    "\n",
    "### Entities\n",
    "Entities are variables used to store text. They are prefaced with & and end with ;.\n",
    "\n",
    "### Comments\n",
    "Comments begin with \\<!-- and end with -->. They are not part of the content. You can think of them as similar to placing a comment after # in Python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a small XML snippet we will be working with*.  I've indented it here so that you can see the structure of the document more clearly. It comes from a dataset of book blurbs which is available through Kaggle.\n",
    "\n",
    "At the 'lowest level' in this document, we have two book elements. The book start tag has two attributes; date and xml:lang. Each of the boook elements has 4 children: title, body, copyright and metadata. metadata has 6 children istelf, and one of them, topics, has a varying number of children again.\n",
    "\n",
    "\\* technically, this is not a valid XML document, as an XML document needs a single root element to be considered well-formed. You will see we are using 'html.parser'; to process well-formed XML you could also use 'xml', but that parser doesn't understand how to deal with our data, as we have multiple root elements. We can use the html.parser because the two formats are so similar."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<book date=\"2018-08-18\" xml:lang=\"en\"> \n",
    "    <title>Bluefish</title>\n",
    "    <body>Thirteen-year-old Travis has a secret: he can’t read. But a shrewd teacher and a sassy girl are about to change everything in this witty and deeply moving novel.Travis is missing his old home in the country, and he’s missing his old hound, Rosco. Now there’s just the cramped place he shares with his well-meaning but alcoholic grandpa, a new school, and the dreaded routine of passing when he’s called on to read out loud. But that’s before Travis meets Mr. McQueen, who doesn’t take “pass” for an answer—a rare teacher whose savvy persistence has Travis slowly unlocking a book on the natural world. And it’s before Travis is noticed by Velveeta, a girl whose wry banter and colorful scarves belie some hard secrets of her own. With sympathy, humor, and disarming honesty, Pat Schmatz brings to life a cast of utterly believable characters—and captures the moments of trust and connection that make all the difference.</body>\n",
    "    <copyright>(c) Penguin Random House</copyright>\n",
    "    <metadata>\n",
    "        <topics>\n",
    "            <d0>Teen & Young Adult</d0>\n",
    "            <d1>Teen & Young Adult Fiction</d1>\n",
    "            <d1>Teen & Young Adult Social Issues</d1>\n",
    "        </topics>\n",
    "        <author>Pat Schmatz</author>\n",
    "        <published>Aug 06, 2013 </published>\n",
    "        <page_num> 240 Pages</page_num>\n",
    "        <isbn>9780763663414</isbn>\n",
    "        <url>https://www.penguinrandomhouse.com/books/214830/bluefish-by-pat-schmatz/</url>\n",
    "    </metadata>\n",
    "</book>\n",
    "<book date=\"2018-08-18\" xml:lang=\"en\"> \n",
    "    <title>Ghost Gone Wild</title>\n",
    "    <body>New York Times bestselling author Carolyn Hart’s good-hearted spirit just can’t say no to an earthly rescue in the fourth Bailey Ruth Ghost novel… As far as emissaries from Heaven’s Department of Good Intentions go, Bailey Ruth is far from the top of the go-to list for assignments in the eyes of her boss, Wiggins. So she’s surprised when she’s dropped off on the outskirts of her old hometown, Adelaide, Oklahoma, at the home of young Nick Magruder. When a window cracks and a rifle barrel is thrust inside, only Bailey Ruth’s hasty intervention saves Nick from taking a bullet. But after she materializes to reassure him, she finds she can’t go back to vanishing. What gives? It turns out Bailey’s been tricked by Nick’s late aunt to come to his rescue, which means Wiggins has no idea where she is—and now she may be trapped in Adelaide forever. Unless she can help snare the person who wants Nick dead…</body>\n",
    "    <copyright>(c) Penguin Random House</copyright>\n",
    "    <metadata>\n",
    "        <topics>\n",
    "            <d2>Cozy Mysteries</d2>\n",
    "            <d0>Fiction</d0>\n",
    "            <d1>Mystery & Suspense</d1>\n",
    "        </topics>\n",
    "        <author>Carolyn Hart</author>\n",
    "        <published>Oct 07, 2014 </published>\n",
    "        <page_num> 304 Pages</page_num>\n",
    "        <isbn>9780425260760</isbn>\n",
    "        <url>https://www.penguinrandomhouse.com/books/312119/ghost-gone-wild-by-carolyn-hart/</url>\n",
    "    </metadata>\n",
    "</book>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BeautifulSoup\n",
    "\n",
    "To process XML using Python, we'll be using a package called Beautiful Soup which provides an xml&html parser. There are other packages available, too. Check if you can `from bs4 import BeautifulSoup` and otherwise install `beautifulsoup4` (do NOT install BeautifulSoup; that is an earlier, now outdated version of the same package). \n",
    "\n",
    "BeautifulSoup documentation: https://www.crummy.com/software/BeautifulSoup/bs4/doc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try to import the package\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the file\n",
    "data = open('blurb_snippet.txt', 'r', encoding=\"utf8\")\n",
    "\n",
    "# parse the data\n",
    "soup = BeautifulSoup(data, 'html.parser')\n",
    "\n",
    "# cose the file\n",
    "data.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we find all 'book'elements in the data, and put them in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "booklist = soup.find_all('book')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we are interested in the book title, the list of topics and the blurb, as well as the date of data collection. Note that this is not the date the book was published - the latter is an element in the metadata, whereas the former is an attribute of the 'book' element. We'll give every book in our dataset an integer as an ID, and put the whole thing in a Python dictionary.\n",
    "\n",
    "We can accesss an element by concatenating the element names in the tree structure intil we arrive at the element we are interested in. \n",
    "\n",
    "If we only want the content of an element, we can use `contents` or `string`. `contents` contains a list - if you know it only ever contains one field, you can unwrap it by indexing the 0th element.\n",
    "\n",
    "If we want all children of an element, regardless of their type, we can use `findChildren()`.\n",
    "\n",
    "If we want to find an attribute, we use `get()` with the name of the attribute as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_data = {} # this will be the big dictionary that contains all our data\n",
    "book_id = 1\n",
    "\n",
    "for book in booklist:\n",
    "    # Title\n",
    "    title = book.title.contents[0] # contents returns a list; we know there is always only 1 title, so we are interested in the element at position 0\n",
    "    \n",
    "    # Topics\n",
    "    topics = []\n",
    "    topictags = book.metadata.topics.findChildren() # we want to find all topics, regardless of the tag\n",
    "    for tag in topictags: \n",
    "        topics.extend(tag.contents) # the tags may contain multiple topics. we add all of them to our list.\n",
    "    \n",
    "    # Blurb\n",
    "    blurb = book.body.string\n",
    "    \n",
    "    # Date of data collection. This is the date that is an attribute of the book opening tag (not the date under 'published')\n",
    "    date_collected = book.get('date')\n",
    "\n",
    "    #put everything in a dictionary, and add it to our big data dictionary with the current book ID as key\n",
    "    book_data[book_id] = {'title' : title,\n",
    "                         'topics' : topics,\n",
    "                         'blurb' : blurb,\n",
    "                         'date_of_data_collection' : date_collected}\n",
    "    \n",
    "    \n",
    "    book_id += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write dictionary to JSON\n",
    "\n",
    "Now let's save our data as JSON for easier future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"book_data.json\", \"w\") as f:\n",
    "    json.dump(book_data, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
